{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"It is important to say that this notebook was taken from lux code for \"stable baselines 3\":\nhttps://github.com/Lux-AI-Challenge/Lux-Design-S2/blob/main/examples/sb3.py\nI made only several changes.\nI hope it will be useful for those who don't know where to find RL model examples.","metadata":{}},{"cell_type":"code","source":"!pip install stable-baselines3\n!pip install --upgrade luxai_s2\n!pip install gym==0.21.0\n!pip install importlib-metadata==4.12.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please, restart your kernel after loading libraries","metadata":{}},{"cell_type":"markdown","source":"Here is the main part of the code for PPO model of stable baselines 3:","metadata":{}},{"cell_type":"code","source":"import copy\nimport os\nimport os.path as osp\nimport gym\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nfrom gym import spaces\nfrom gym.wrappers import TimeLimit\nfrom stable_baselines3.common.callbacks import BaseCallback, EvalCallback\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.utils import set_random_seed\nfrom stable_baselines3.common.vec_env import (\n    DummyVecEnv,\n    SubprocVecEnv,\n    VecCheckNan,\n    VecVideoRecorder,\n)\nfrom stable_baselines3.ppo import PPO\n\nfrom luxai_s2.state import ObservationStateDict, StatsStateDict, create_empty_stats\nfrom luxai_s2.utils.heuristics.factory import build_single_heavy\nfrom luxai_s2.utils.heuristics.factory_placement import place_near_random_ice\nfrom luxai_s2.wrappers import (\n    SB3Wrapper,\n    SimpleSingleUnitDiscreteController,\n    SingleUnitObservationWrapper,\n)\n#os.chdir('/kaggle/input')\n\nclass CustomEnvWrapper(gym.Wrapper):\n    def __init__(self, env: gym.Env) -> None:\n        \"\"\"\n        Adds a custom reward and turns the LuxAI_S2 environment into a single-agent environment for easy training\n        \"\"\"\n        super().__init__(env)\n        self.prev_step_metrics = None\n\n    def step(self, action):\n        agent = \"player_0\"\n        opp_agent = \"player_1\"\n\n        opp_factories = self.env.state.factories[opp_agent]\n        for k in opp_factories:\n            factory = opp_factories[k]\n            factory.cargo.water = 1000 # set enemy factories to have 1000 water to keep them alive the whole around and treat the game as single-agent\n\n        action = {agent: action}\n        obs, reward, done, info = super().step(action)\n\n        # this is the observation seen by both agents\n        shared_obs: ObservationStateDict = self.env.prev_obs[agent]\n        done = done[agent]\n\n        # we collect stats on teams here:\n        stats: StatsStateDict = self.env.state.stats[agent]\n        \n        # compute reward\n        # we simply want to encourage the heavy units to move to ice tiles\n        # and mine them and then bring them back to the factory and dump it\n        # as well as survive as long as possible\n\n        factories = shared_obs[\"factories\"][agent]\n        factory_pos = None\n        for unit_id in factories:\n            factory = factories[unit_id]\n            # note that ice converts to water at a 4:1 ratio\n            factory_pos = np.array(factory[\"pos\"])\n            break\n        units = shared_obs[\"units\"][agent]\n        unit_deliver_ice_reward = 0\n        unit_move_to_ice_reward = 0\n        unit_overmining_penalty = 0\n        penalize_power_waste = 0\n\n        ice_map = shared_obs[\"board\"][\"ice\"]\n        ice_tile_locations = np.argwhere(ice_map == 1)\n\n        def manhattan_dist(p1, p2):\n            return abs(p1[0] - p2[0]) + abs(p1[1] - p2[1])\n\n        unit_power = 0\n        for unit_id in units:\n            unit = units[unit_id]\n            if unit[\"unit_type\"] == \"HEAVY\":\n                pos = np.array(unit[\"pos\"])\n                ice_tile_distances = np.mean((ice_tile_locations - pos) ** 2, 1)\n                closest_ice_tile = ice_tile_locations[np.argmin(ice_tile_distances)]\n                dist_to_ice = manhattan_dist(closest_ice_tile, pos)\n                unit_power = unit[\"power\"]\n                if unit[\"cargo\"][\"ice\"] < 20:\n\n                    dist_penalty = min(\n                        1.0, dist_to_ice / (10)\n                    )  # go beyond 12 squares manhattan dist and no reward\n                    unit_move_to_ice_reward += (\n                        1 - dist_penalty\n                    ) * 0.1  # encourage unit to move to ice\n                else:\n                    if factory_pos is not None:\n                        dist_to_factory = manhattan_dist(pos, factory_pos)\n                        dist_penalty = min(1.0, dist_to_factory / 10)\n                        unit_deliver_ice_reward = (\n                            0.2 + (1 - dist_penalty) * 0.1\n                        )  # encourage unit to move back to factory\n                if action[agent] == 15 and unit[\"power\"] < 70:\n                    # penalize the agent for trying to dig with insufficient power, which wastes 10 power for trying to update the action queue\n                    penalize_power_waste -= 0.005\n\n        # save some stats to the info object so we can record it with our SB3 logger\n        info = dict()\n        metrics = dict()\n        metrics[\"ice_dug\"] = (\n            stats[\"generation\"][\"ice\"][\"HEAVY\"] + stats[\"generation\"][\"ice\"][\"LIGHT\"]\n        )\n        metrics[\"water_produced\"] = stats[\"generation\"][\"water\"]\n        metrics[\"action_queue_updates_success\"] = stats[\"action_queue_updates_success\"]\n        metrics[\"action_queue_updates_total\"] = stats[\"action_queue_updates_total\"]\n\n        metrics[\"unit_deliver_ice_reward\"] = unit_deliver_ice_reward\n        metrics[\"unit_move_to_ice_reward\"] = unit_move_to_ice_reward\n\n        info[\"metrics\"] = metrics\n\n        reward = (\n            0\n            + unit_move_to_ice_reward\n            + unit_deliver_ice_reward\n            + unit_overmining_penalty\n            + metrics[\"water_produced\"] / 10 + penalize_power_waste\n        )\n        reward = reward\n        if self.prev_step_metrics is not None:\n            ice_dug_this_step = metrics[\"ice_dug\"] - self.prev_step_metrics[\"ice_dug\"]\n            water_produced_this_step = (\n                metrics[\"water_produced\"] - self.prev_step_metrics[\"water_produced\"]\n            )\n            # reward += ice_dug_this_step # reward agent for digging ice\n            # reward += water_produced_this_step * 100 # reward agent even more producing water by delivering ice back to base\n        self.prev_step_metrics = copy.deepcopy(metrics)\n        return obs[\"player_0\"], reward, done, info\n\n    def reset(self, **kwargs):\n        obs = self.env.reset(**kwargs)[\"player_0\"]\n        self.prev_step_metrics = None\n        return obs\n\n\nclass Parse_args:\n    def __init__(self):\n        self.max_episode_steps = 100\n        self.seed = 13\n        self.n_envs = 8\n        self.total_timesteps = 2000000\n        self.log_path = \"logs\"\n        self.model_path = '/kaggle/working/latest_model'\n        self.eval = False\n    \n    \ndef make_env(env_id: str, rank: int, seed: int = 0, max_episode_steps=100):\n    def _init() -> gym.Env:\n        # verbose = 0\n        # collect stats so we can create reward functions\n        # max factories set to 2 for simplification and keeping returns consistent as we survive longer if there are more initial resources\n        env = gym.make(env_id, verbose=0, collect_stats=True, MAX_FACTORIES=2)\n\n        # Add a SB3 wrapper to make it work with SB3 and simplify the action space with the controller\n        # this will remove the bidding phase and factory placement phase. For factory placement we use\n        # the provided place_near_random_ice function which will randomly select an ice tile and place a factory near it.\n        env = SB3Wrapper(\n            env,\n            controller=SimpleSingleUnitDiscreteController(env.state.env_cfg),\n            factory_placement_policy=place_near_random_ice,\n            heuristic_policy=build_single_heavy,\n        )\n        env = SingleUnitObservationWrapper(\n            env\n        )  # changes observation to include a few simple features\n        env = CustomEnvWrapper(env)  # convert to single agent and add our reward\n        env = TimeLimit(\n            env, max_episode_steps=max_episode_steps\n        )  # set horizon to 100 to make training faster. Default is 1000\n        env = Monitor(env)  # for SB3 to allow it to record metrics\n        env.reset(seed=seed + rank)\n        set_random_seed(seed)\n        return env\n\n    return _init\n\n\nenv_id = \"LuxAI_S2-v0\"\n\nfrom collections import defaultdict\n\n\nclass TensorboardCallback(BaseCallback):\n    def __init__(self, tag: str, verbose=0):\n        super().__init__(verbose)\n        self.tag = tag\n\n    def _on_step(self) -> bool:\n        c = 0\n        \n        for i, done in enumerate(self.locals[\"dones\"]):\n            if done:\n                info = self.locals[\"infos\"][i]\n                c += 1\n                for k in info[\"metrics\"]:\n                    stat = info[\"metrics\"][k]\n                    self.logger.record_mean(f\"{self.tag}/{k}\", stat)\n        return True\n\n\ndef evaluate(args, model,):\n    model = model.load(args.model_path)\n    video_length = 1000  # default horizon\n    eval_env = SubprocVecEnv([make_env(env_id, i, max_episode_steps=1000) for i in range(args.n_envs)])\n    eval_env = VecVideoRecorder(\n        eval_env,\n        osp.join(args.log_path, \"eval_videos\"),\n        record_video_trigger=lambda x: x == 0,\n        video_length=video_length,\n        name_prefix=f\"evaluation_video\",\n    )\n    eval_env.reset()\n    out =evaluate_policy(model, eval_env, render=False, deterministic=False)\n    print(out)\n\ndef train(args, model: PPO,):\n    eval_env = SubprocVecEnv([make_env(env_id, i, max_episode_steps=1000) for i in range(4)])\n    video_length = 1000\n    eval_env = VecVideoRecorder(\n        eval_env,\n        osp.join(args.log_path, \"eval_videos\"),\n        record_video_trigger=lambda x: x == 0,\n        video_length=video_length,\n        name_prefix=f\"evaluation-{env_id}\",\n    )\n    eval_callback = EvalCallback(\n        eval_env,\n        best_model_save_path=osp.join(args.log_path, \"models\"),\n        log_path=osp.join(args.log_path, \"eval_logs\"),\n        eval_freq=24_000,\n        deterministic=False,\n        render=False,\n    )\n    model.learn(\n        args.total_timesteps,\n        callback=[TensorboardCallback(tag=\"train_metrics\"), eval_callback],\n    )\n    \n    model.save(args.log_path, \"latest_model\")","metadata":{"execution":{"iopub.status.busy":"2023-01-31T18:27:03.675333Z","iopub.execute_input":"2023-01-31T18:27:03.675808Z","iopub.status.idle":"2023-01-31T18:27:05.335711Z","shell.execute_reply.started":"2023-01-31T18:27:03.675722Z","shell.execute_reply":"2023-01-31T18:27:05.334710Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Here is the main function which creates your PPO model:","metadata":{}},{"cell_type":"code","source":"def main(args):\n    print(\"Training with args\", args)\n    set_random_seed(args.seed)\n    env = SubprocVecEnv([make_env(env_id, i, max_episode_steps=args.max_episode_steps) for i in range(args.n_envs)])\n    env.reset()\n    rollout_steps = 4_000\n    policy_kwargs = dict(net_arch=(128, 128))\n    model = PPO(\n        \"MlpPolicy\",\n        env,\n        n_steps=rollout_steps // args.n_envs,\n        batch_size=800,\n        learning_rate=1e-3,\n        policy_kwargs=policy_kwargs,\n        verbose=1,\n        n_epochs=3,\n        target_kl=0.07,\n        gamma=0.97,\n        device=th.device('cuda'),\n        tensorboard_log=osp.join(args.log_path),\n    )\n    if args.eval:\n        evaluate(args, model)\n    else:\n        train(args, model)\n\nif __name__ == \"__main__\":\n    parse_args = Parse_args()\n    main(parse_args)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T18:27:06.157707Z","iopub.execute_input":"2023-01-31T18:27:06.158389Z","iopub.status.idle":"2023-01-31T18:46:02.722277Z","shell.execute_reply.started":"2023-01-31T18:27:06.158348Z","shell.execute_reply":"2023-01-31T18:46:02.720648Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Training with args <__main__.Parse_args object at 0x7f0eb96c11d0>\nUsing cuda device\nLogging to logs/PPO_8\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f0eb8e5f650> != <stable_baselines3.common.vec_env.vec_video_recorder.VecVideoRecorder object at 0x7f0de685e990>\n  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n","output_type":"stream"},{"name":"stdout","text":"----------------------------------------------\n| rollout/                        |          |\n|    ep_len_mean                  | 35.1     |\n|    ep_rew_mean                  | 2.48     |\n| time/                           |          |\n|    fps                          | 534      |\n|    iterations                   | 1        |\n|    time_elapsed                 | 7        |\n|    total_timesteps              | 4000     |\n| train_metrics/                  |          |\n|    action_queue_updates_success | 28.2     |\n|    action_queue_updates_total   | 32.8     |\n|    ice_dug                      | 0.571    |\n|    unit_deliver_ice_reward      | 0        |\n|    unit_move_to_ice_reward      | 0.0701   |\n|    water_produced               | 0        |\n----------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 54.8         |\n|    ep_rew_mean                  | 3.85         |\n| time/                           |              |\n|    fps                          | 644          |\n|    iterations                   | 2            |\n|    time_elapsed                 | 12           |\n|    total_timesteps              | 8000         |\n| train/                          |              |\n|    approx_kl                    | 0.0073825913 |\n|    clip_fraction                | 0.0327       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.77        |\n|    explained_variance           | 0.00267      |\n|    learning_rate                | 0.001        |\n|    loss                         | 0.057        |\n|    n_updates                    | 3            |\n|    policy_gradient_loss         | -0.00868     |\n|    value_loss                   | 0.2          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 53           |\n|    action_queue_updates_total   | 64.2         |\n|    ice_dug                      | 0            |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0677       |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 61.1         |\n|    ep_rew_mean                  | 4.24         |\n| time/                           |              |\n|    fps                          | 660          |\n|    iterations                   | 3            |\n|    time_elapsed                 | 18           |\n|    total_timesteps              | 12000        |\n| train/                          |              |\n|    approx_kl                    | 0.0044444134 |\n|    clip_fraction                | 0.00525      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.75        |\n|    explained_variance           | -0.122       |\n|    learning_rate                | 0.001        |\n|    loss                         | 0.0475       |\n|    n_updates                    | 6            |\n|    policy_gradient_loss         | -0.00436     |\n|    value_loss                   | 0.179        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 49.4         |\n|    action_queue_updates_total   | 58.4         |\n|    ice_dug                      | 0            |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0703       |\n|    water_produced               | 0            |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 63.4        |\n|    ep_rew_mean                  | 4.29        |\n| time/                           |             |\n|    fps                          | 698         |\n|    iterations                   | 4           |\n|    time_elapsed                 | 22          |\n|    total_timesteps              | 16000       |\n| train/                          |             |\n|    approx_kl                    | 0.006193042 |\n|    clip_fraction                | 0.0332      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.73       |\n|    explained_variance           | -0.0482     |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.0789      |\n|    n_updates                    | 9           |\n|    policy_gradient_loss         | -0.00391    |\n|    value_loss                   | 0.206       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 48.3        |\n|    action_queue_updates_total   | 65.2        |\n|    ice_dug                      | 0           |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0678      |\n|    water_produced               | 0           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 69.9         |\n|    ep_rew_mean                  | 4.93         |\n| time/                           |              |\n|    fps                          | 713          |\n|    iterations                   | 5            |\n|    time_elapsed                 | 28           |\n|    total_timesteps              | 20000        |\n| train/                          |              |\n|    approx_kl                    | 0.0069557494 |\n|    clip_fraction                | 0.0384       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.71        |\n|    explained_variance           | -0.149       |\n|    learning_rate                | 0.001        |\n|    loss                         | 0.055        |\n|    n_updates                    | 12           |\n|    policy_gradient_loss         | -0.00516     |\n|    value_loss                   | 0.141        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 54.8         |\n|    action_queue_updates_total   | 73.7         |\n|    ice_dug                      | 0.377        |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0685       |\n|    water_produced               | 0            |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 80.2        |\n|    ep_rew_mean                  | 5.85        |\n| time/                           |             |\n|    fps                          | 732         |\n|    iterations                   | 6           |\n|    time_elapsed                 | 32          |\n|    total_timesteps              | 24000       |\n| train/                          |             |\n|    approx_kl                    | 0.007308206 |\n|    clip_fraction                | 0.0563      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.66       |\n|    explained_variance           | -0.159      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.0624      |\n|    n_updates                    | 15          |\n|    policy_gradient_loss         | -0.00423    |\n|    value_loss                   | 0.159       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 59.8        |\n|    action_queue_updates_total   | 85.4        |\n|    ice_dug                      | 0           |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0691      |\n|    water_produced               | 0           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 89.7         |\n|    ep_rew_mean                  | 6.48         |\n| time/                           |              |\n|    fps                          | 741          |\n|    iterations                   | 7            |\n|    time_elapsed                 | 37           |\n|    total_timesteps              | 28000        |\n| train/                          |              |\n|    approx_kl                    | 0.0043203486 |\n|    clip_fraction                | 0.0161       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.65        |\n|    explained_variance           | -0.36        |\n|    learning_rate                | 0.001        |\n|    loss                         | 0.0416       |\n|    n_updates                    | 18           |\n|    policy_gradient_loss         | -0.00187     |\n|    value_loss                   | 0.105        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 69.2         |\n|    action_queue_updates_total   | 94.1         |\n|    ice_dug                      | 0            |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0654       |\n|    water_produced               | 0            |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 92.5        |\n|    ep_rew_mean                  | 6.73        |\n| time/                           |             |\n|    fps                          | 752         |\n|    iterations                   | 8           |\n|    time_elapsed                 | 42          |\n|    total_timesteps              | 32000       |\n| train/                          |             |\n|    approx_kl                    | 0.004806719 |\n|    clip_fraction                | 0.0291      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.65       |\n|    explained_variance           | -0.378      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.0422      |\n|    n_updates                    | 21          |\n|    policy_gradient_loss         | -0.00228    |\n|    value_loss                   | 0.0928      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 69.1        |\n|    action_queue_updates_total   | 90.4        |\n|    ice_dug                      | 0           |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0677      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 91.7        |\n|    ep_rew_mean                  | 6.69        |\n| time/                           |             |\n|    fps                          | 753         |\n|    iterations                   | 9           |\n|    time_elapsed                 | 47          |\n|    total_timesteps              | 36000       |\n| train/                          |             |\n|    approx_kl                    | 0.004368598 |\n|    clip_fraction                | 0.0277      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.65       |\n|    explained_variance           | -0.226      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.045       |\n|    n_updates                    | 24          |\n|    policy_gradient_loss         | -0.00317    |\n|    value_loss                   | 0.112       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 66.6        |\n|    action_queue_updates_total   | 90.7        |\n|    ice_dug                      | 0.93        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0702      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 91.9        |\n|    ep_rew_mean                  | 6.33        |\n| time/                           |             |\n|    fps                          | 757         |\n|    iterations                   | 10          |\n|    time_elapsed                 | 52          |\n|    total_timesteps              | 40000       |\n| train/                          |             |\n|    approx_kl                    | 0.006885869 |\n|    clip_fraction                | 0.0158      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.64       |\n|    explained_variance           | -0.203      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.046       |\n|    n_updates                    | 27          |\n|    policy_gradient_loss         | -0.00293    |\n|    value_loss                   | 0.088       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 64.1        |\n|    action_queue_updates_total   | 90.5        |\n|    ice_dug                      | 0.455       |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0573      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 93.6        |\n|    ep_rew_mean                  | 6.13        |\n| time/                           |             |\n|    fps                          | 765         |\n|    iterations                   | 11          |\n|    time_elapsed                 | 57          |\n|    total_timesteps              | 44000       |\n| train/                          |             |\n|    approx_kl                    | 0.004013308 |\n|    clip_fraction                | 0.0132      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.63       |\n|    explained_variance           | -0.295      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.0335      |\n|    n_updates                    | 30          |\n|    policy_gradient_loss         | -0.00137    |\n|    value_loss                   | 0.082       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 70.7        |\n|    action_queue_updates_total   | 92.6        |\n|    ice_dug                      | 0.476       |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0602      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 94.5        |\n|    ep_rew_mean                  | 6.53        |\n| time/                           |             |\n|    fps                          | 769         |\n|    iterations                   | 12          |\n|    time_elapsed                 | 62          |\n|    total_timesteps              | 48000       |\n| train/                          |             |\n|    approx_kl                    | 0.008732511 |\n|    clip_fraction                | 0.055       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.61       |\n|    explained_variance           | -0.398      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.0392      |\n|    n_updates                    | 33          |\n|    policy_gradient_loss         | -0.00305    |\n|    value_loss                   | 0.0917      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 67.8        |\n|    action_queue_updates_total   | 94.4        |\n|    ice_dug                      | 1.9         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0719      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 95.1        |\n|    ep_rew_mean                  | 6.97        |\n| time/                           |             |\n|    fps                          | 776         |\n|    iterations                   | 13          |\n|    time_elapsed                 | 66          |\n|    total_timesteps              | 52000       |\n| train/                          |             |\n|    approx_kl                    | 0.011135438 |\n|    clip_fraction                | 0.0787      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.6        |\n|    explained_variance           | -0.299      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.0155      |\n|    n_updates                    | 36          |\n|    policy_gradient_loss         | -0.00375    |\n|    value_loss                   | 0.0656      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 73.2        |\n|    action_queue_updates_total   | 96.2        |\n|    ice_dug                      | 0           |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0676      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 98.8        |\n|    ep_rew_mean                  | 7.47        |\n| time/                           |             |\n|    fps                          | 779         |\n|    iterations                   | 14          |\n|    time_elapsed                 | 71          |\n|    total_timesteps              | 56000       |\n| train/                          |             |\n|    approx_kl                    | 0.010844241 |\n|    clip_fraction                | 0.066       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.56       |\n|    explained_variance           | -0.0274     |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.241       |\n|    n_updates                    | 39          |\n|    policy_gradient_loss         | -0.00309    |\n|    value_loss                   | 0.553       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 73.5        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 0.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.063       |\n|    water_produced               | 0.125       |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 96.5        |\n|    ep_rew_mean                  | 7.29        |\n| time/                           |             |\n|    fps                          | 784         |\n|    iterations                   | 15          |\n|    time_elapsed                 | 76          |\n|    total_timesteps              | 60000       |\n| train/                          |             |\n|    approx_kl                    | 0.014351708 |\n|    clip_fraction                | 0.143       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.56       |\n|    explained_variance           | 0.0087      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.0524      |\n|    n_updates                    | 42          |\n|    policy_gradient_loss         | -0.00409    |\n|    value_loss                   | 0.138       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 66.9        |\n|    action_queue_updates_total   | 90.8        |\n|    ice_dug                      | 0           |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0726      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 95.5        |\n|    ep_rew_mean                  | 7.07        |\n| time/                           |             |\n|    fps                          | 779         |\n|    iterations                   | 16          |\n|    time_elapsed                 | 82          |\n|    total_timesteps              | 64000       |\n| train/                          |             |\n|    approx_kl                    | 0.011031016 |\n|    clip_fraction                | 0.054       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.55       |\n|    explained_variance           | 0.0893      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.036       |\n|    n_updates                    | 45          |\n|    policy_gradient_loss         | -0.00337    |\n|    value_loss                   | 0.077       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 66.4        |\n|    action_queue_updates_total   | 96.7        |\n|    ice_dug                      | 0.488       |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0785      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 96.6        |\n|    ep_rew_mean                  | 7.14        |\n| time/                           |             |\n|    fps                          | 785         |\n|    iterations                   | 17          |\n|    time_elapsed                 | 86          |\n|    total_timesteps              | 68000       |\n| train/                          |             |\n|    approx_kl                    | 0.010770926 |\n|    clip_fraction                | 0.0803      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.54       |\n|    explained_variance           | 0.0589      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.00685     |\n|    n_updates                    | 48          |\n|    policy_gradient_loss         | -0.00345    |\n|    value_loss                   | 0.0423      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 67.4        |\n|    action_queue_updates_total   | 95.4        |\n|    ice_dug                      | 0           |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0695      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 97.5        |\n|    ep_rew_mean                  | 7.03        |\n| time/                           |             |\n|    fps                          | 788         |\n|    iterations                   | 18          |\n|    time_elapsed                 | 91          |\n|    total_timesteps              | 72000       |\n| train/                          |             |\n|    approx_kl                    | 0.011137388 |\n|    clip_fraction                | 0.116       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.53       |\n|    explained_variance           | -0.00265    |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.0188      |\n|    n_updates                    | 51          |\n|    policy_gradient_loss         | -0.00437    |\n|    value_loss                   | 0.0564      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 67.5        |\n|    action_queue_updates_total   | 96.7        |\n|    ice_dug                      | 0           |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0671      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 98.4        |\n|    ep_rew_mean                  | 7.08        |\n| time/                           |             |\n|    fps                          | 790         |\n|    iterations                   | 19          |\n|    time_elapsed                 | 96          |\n|    total_timesteps              | 76000       |\n| train/                          |             |\n|    approx_kl                    | 0.008529561 |\n|    clip_fraction                | 0.0713      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.52       |\n|    explained_variance           | 0.277       |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.0137      |\n|    n_updates                    | 54          |\n|    policy_gradient_loss         | -0.00288    |\n|    value_loss                   | 0.056       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 68.1        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 0           |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.071       |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 7.3         |\n| time/                           |             |\n|    fps                          | 793         |\n|    iterations                   | 20          |\n|    time_elapsed                 | 100         |\n|    total_timesteps              | 80000       |\n| train/                          |             |\n|    approx_kl                    | 0.010898004 |\n|    clip_fraction                | 0.084       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.52       |\n|    explained_variance           | 0.186       |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.00913     |\n|    n_updates                    | 57          |\n|    policy_gradient_loss         | -0.00391    |\n|    value_loss                   | 0.0353      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 66.6        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 0.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0695      |\n|    water_produced               | 0           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 99.1         |\n|    ep_rew_mean                  | 8.05         |\n| time/                           |              |\n|    fps                          | 793          |\n|    iterations                   | 21           |\n|    time_elapsed                 | 105          |\n|    total_timesteps              | 84000        |\n| train/                          |              |\n|    approx_kl                    | 0.0073227794 |\n|    clip_fraction                | 0.0481       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.52        |\n|    explained_variance           | -0.423       |\n|    learning_rate                | 0.001        |\n|    loss                         | 0.0158       |\n|    n_updates                    | 60           |\n|    policy_gradient_loss         | -0.0021      |\n|    value_loss                   | 0.0439       |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 67.6         |\n|    action_queue_updates_total   | 96.8         |\n|    ice_dug                      | 1.46         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.072        |\n|    water_produced               | 0.244        |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 99.1         |\n|    ep_rew_mean                  | 8.11         |\n| time/                           |              |\n|    fps                          | 792          |\n|    iterations                   | 22           |\n|    time_elapsed                 | 111          |\n|    total_timesteps              | 88000        |\n| train/                          |              |\n|    approx_kl                    | 0.0061616087 |\n|    clip_fraction                | 0.0262       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.5         |\n|    explained_variance           | 0.0105       |\n|    learning_rate                | 0.001        |\n|    loss                         | 0.597        |\n|    n_updates                    | 63           |\n|    policy_gradient_loss         | -0.00269     |\n|    value_loss                   | 1.1          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 68.9         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 0            |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0737       |\n|    water_produced               | 0            |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 99.5        |\n|    ep_rew_mean                  | 7.91        |\n| time/                           |             |\n|    fps                          | 794         |\n|    iterations                   | 23          |\n|    time_elapsed                 | 115         |\n|    total_timesteps              | 92000       |\n| train/                          |             |\n|    approx_kl                    | 0.013222875 |\n|    clip_fraction                | 0.0827      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.54       |\n|    explained_variance           | 0.423       |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.011       |\n|    n_updates                    | 66          |\n|    policy_gradient_loss         | -0.0033     |\n|    value_loss                   | 0.0467      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 68.6        |\n|    action_queue_updates_total   | 97.8        |\n|    ice_dug                      | 0.976       |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0763      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 99.5        |\n|    ep_rew_mean                  | 7.6         |\n| time/                           |             |\n|    fps                          | 795         |\n|    iterations                   | 24          |\n|    time_elapsed                 | 120         |\n|    total_timesteps              | 96000       |\n| train/                          |             |\n|    approx_kl                    | 0.011683108 |\n|    clip_fraction                | 0.0792      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.53       |\n|    explained_variance           | 0.122       |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.015       |\n|    n_updates                    | 69          |\n|    policy_gradient_loss         | -0.00378    |\n|    value_loss                   | 0.0528      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 72.8        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 2.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0737      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 99.5        |\n|    ep_rew_mean                  | 7.97        |\n| time/                           |             |\n|    fps                          | 798         |\n|    iterations                   | 25          |\n|    time_elapsed                 | 125         |\n|    total_timesteps              | 100000      |\n| train/                          |             |\n|    approx_kl                    | 0.006537662 |\n|    clip_fraction                | 0.0317      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.51       |\n|    explained_variance           | -0.136      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.0101      |\n|    n_updates                    | 72          |\n|    policy_gradient_loss         | -0.00148    |\n|    value_loss                   | 0.0332      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 70.4        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 2.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0762      |\n|    water_produced               | 0.125       |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 8.05        |\n| time/                           |             |\n|    fps                          | 799         |\n|    iterations                   | 26          |\n|    time_elapsed                 | 130         |\n|    total_timesteps              | 104000      |\n| train/                          |             |\n|    approx_kl                    | 0.007412532 |\n|    clip_fraction                | 0.0427      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.5        |\n|    explained_variance           | 0.0698      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.447       |\n|    n_updates                    | 75          |\n|    policy_gradient_loss         | -0.00184    |\n|    value_loss                   | 0.646       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 76.1        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 2.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0773      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 8.36        |\n| time/                           |             |\n|    fps                          | 801         |\n|    iterations                   | 27          |\n|    time_elapsed                 | 134         |\n|    total_timesteps              | 108000      |\n| train/                          |             |\n|    approx_kl                    | 0.012427311 |\n|    clip_fraction                | 0.0951      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.5        |\n|    explained_variance           | 0.146       |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.0116      |\n|    n_updates                    | 78          |\n|    policy_gradient_loss         | -0.00397    |\n|    value_loss                   | 0.0481      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 72.3        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 2.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.07        |\n|    water_produced               | 0.25        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 8.4          |\n| time/                           |              |\n|    fps                          | 801          |\n|    iterations                   | 28           |\n|    time_elapsed                 | 139          |\n|    total_timesteps              | 112000       |\n| train/                          |              |\n|    approx_kl                    | 0.0085551515 |\n|    clip_fraction                | 0.0493       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.5         |\n|    explained_variance           | 0.053        |\n|    learning_rate                | 0.001        |\n|    loss                         | 1.07         |\n|    n_updates                    | 81           |\n|    policy_gradient_loss         | 0.000712     |\n|    value_loss                   | 2.42         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 71.9         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 1.5          |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0765       |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 8.3          |\n| time/                           |              |\n|    fps                          | 800          |\n|    iterations                   | 29           |\n|    time_elapsed                 | 144          |\n|    total_timesteps              | 116000       |\n| train/                          |              |\n|    approx_kl                    | 0.0058516124 |\n|    clip_fraction                | 0.0317       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.5         |\n|    explained_variance           | 0.403        |\n|    learning_rate                | 0.001        |\n|    loss                         | 0.0113       |\n|    n_updates                    | 84           |\n|    policy_gradient_loss         | -0.00315     |\n|    value_loss                   | 0.0445       |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 74.3         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 4.5          |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0775       |\n|    water_produced               | 0.25         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 8.71        |\n| time/                           |             |\n|    fps                          | 801         |\n|    iterations                   | 30          |\n|    time_elapsed                 | 149         |\n|    total_timesteps              | 120000      |\n| train/                          |             |\n|    approx_kl                    | 0.010742708 |\n|    clip_fraction                | 0.0515      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.5        |\n|    explained_variance           | 0.0601      |\n|    learning_rate                | 0.001       |\n|    loss                         | 1.12        |\n|    n_updates                    | 87          |\n|    policy_gradient_loss         | -0.0032     |\n|    value_loss                   | 1.69        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 72.5        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 4           |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0743      |\n|    water_produced               | 0.125       |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 99           |\n|    ep_rew_mean                  | 8.86         |\n| time/                           |              |\n|    fps                          | 803          |\n|    iterations                   | 31           |\n|    time_elapsed                 | 154          |\n|    total_timesteps              | 124000       |\n| train/                          |              |\n|    approx_kl                    | 0.0063356147 |\n|    clip_fraction                | 0.0573       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.51        |\n|    explained_variance           | 0.508        |\n|    learning_rate                | 0.001        |\n|    loss                         | 0.0864       |\n|    n_updates                    | 90           |\n|    policy_gradient_loss         | -0.00237     |\n|    value_loss                   | 0.206        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 74.3         |\n|    action_queue_updates_total   | 96.7         |\n|    ice_dug                      | 1.46         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0802       |\n|    water_produced               | 0.122        |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 99          |\n|    ep_rew_mean                  | 8.33        |\n| time/                           |             |\n|    fps                          | 803         |\n|    iterations                   | 32          |\n|    time_elapsed                 | 159         |\n|    total_timesteps              | 128000      |\n| train/                          |             |\n|    approx_kl                    | 0.003263967 |\n|    clip_fraction                | 0.00567     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.5        |\n|    explained_variance           | 0.0286      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.199       |\n|    n_updates                    | 93          |\n|    policy_gradient_loss         | -0.00156    |\n|    value_loss                   | 0.359       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 70.4        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 3.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0737      |\n|    water_produced               | 0.125       |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 99          |\n|    ep_rew_mean                  | 8.46        |\n| time/                           |             |\n|    fps                          | 804         |\n|    iterations                   | 33          |\n|    time_elapsed                 | 163         |\n|    total_timesteps              | 132000      |\n| train/                          |             |\n|    approx_kl                    | 0.010392734 |\n|    clip_fraction                | 0.0573      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.5        |\n|    explained_variance           | -0.0699     |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.296       |\n|    n_updates                    | 96          |\n|    policy_gradient_loss         | -0.00299    |\n|    value_loss                   | 0.725       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 70.5        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 0.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0782      |\n|    water_produced               | 0.125       |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 8.48        |\n| time/                           |             |\n|    fps                          | 805         |\n|    iterations                   | 34          |\n|    time_elapsed                 | 168         |\n|    total_timesteps              | 136000      |\n| train/                          |             |\n|    approx_kl                    | 0.011564565 |\n|    clip_fraction                | 0.0627      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.48       |\n|    explained_variance           | -0.0088     |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.347       |\n|    n_updates                    | 99          |\n|    policy_gradient_loss         | -0.00194    |\n|    value_loss                   | 0.618       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 76.8        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 1           |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0733      |\n|    water_produced               | 0           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 7.67         |\n| time/                           |              |\n|    fps                          | 803          |\n|    iterations                   | 35           |\n|    time_elapsed                 | 174          |\n|    total_timesteps              | 140000       |\n| train/                          |              |\n|    approx_kl                    | 0.0055606533 |\n|    clip_fraction                | 0.0422       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.45        |\n|    explained_variance           | 0.274        |\n|    learning_rate                | 0.001        |\n|    loss                         | 0.0111       |\n|    n_updates                    | 102          |\n|    policy_gradient_loss         | -0.00219     |\n|    value_loss                   | 0.0358       |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 73.7         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 1            |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0755       |\n|    water_produced               | 0            |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 7.82        |\n| time/                           |             |\n|    fps                          | 805         |\n|    iterations                   | 36          |\n|    time_elapsed                 | 178         |\n|    total_timesteps              | 144000      |\n| train/                          |             |\n|    approx_kl                    | 0.011203678 |\n|    clip_fraction                | 0.0608      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.39       |\n|    explained_variance           | 0.191       |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.013       |\n|    n_updates                    | 105         |\n|    policy_gradient_loss         | -0.00274    |\n|    value_loss                   | 0.0304      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 80.2        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 2           |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0797      |\n|    water_produced               | 0           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 8.31        |\n| time/                           |             |\n|    fps                          | 805         |\n|    iterations                   | 37          |\n|    time_elapsed                 | 183         |\n|    total_timesteps              | 148000      |\n| train/                          |             |\n|    approx_kl                    | 0.010168305 |\n|    clip_fraction                | 0.0522      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.37       |\n|    explained_variance           | -0.0654     |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.228       |\n|    n_updates                    | 108         |\n|    policy_gradient_loss         | -0.00212    |\n|    value_loss                   | 0.542       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 71.4        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 2.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0733      |\n|    water_produced               | 0.125       |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 8.62        |\n| time/                           |             |\n|    fps                          | 806         |\n|    iterations                   | 38          |\n|    time_elapsed                 | 188         |\n|    total_timesteps              | 152000      |\n| train/                          |             |\n|    approx_kl                    | 0.010792984 |\n|    clip_fraction                | 0.0742      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.42       |\n|    explained_variance           | -0.0606     |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.0473      |\n|    n_updates                    | 111         |\n|    policy_gradient_loss         | -0.00309    |\n|    value_loss                   | 0.116       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 70.8        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 2.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0715      |\n|    water_produced               | 0.125       |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 9.15         |\n| time/                           |              |\n|    fps                          | 807          |\n|    iterations                   | 39           |\n|    time_elapsed                 | 193          |\n|    total_timesteps              | 156000       |\n| train/                          |              |\n|    approx_kl                    | 0.0073775165 |\n|    clip_fraction                | 0.0418       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.41        |\n|    explained_variance           | 0.0214       |\n|    learning_rate                | 0.001        |\n|    loss                         | 0.308        |\n|    n_updates                    | 114          |\n|    policy_gradient_loss         | -0.00181     |\n|    value_loss                   | 0.742        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 72.5         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 3.5          |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.074        |\n|    water_produced               | 0.375        |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 9.02        |\n| time/                           |             |\n|    fps                          | 808         |\n|    iterations                   | 40          |\n|    time_elapsed                 | 198         |\n|    total_timesteps              | 160000      |\n| train/                          |             |\n|    approx_kl                    | 0.006108351 |\n|    clip_fraction                | 0.0457      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.4        |\n|    explained_variance           | 0.00679     |\n|    learning_rate                | 0.001       |\n|    loss                         | 1.23        |\n|    n_updates                    | 117         |\n|    policy_gradient_loss         | -0.00158    |\n|    value_loss                   | 2.99        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 69.5        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 1.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0675      |\n|    water_produced               | 0.125       |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 9.28        |\n| time/                           |             |\n|    fps                          | 807         |\n|    iterations                   | 41          |\n|    time_elapsed                 | 203         |\n|    total_timesteps              | 164000      |\n| train/                          |             |\n|    approx_kl                    | 0.007397706 |\n|    clip_fraction                | 0.0253      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.38       |\n|    explained_variance           | 0.127       |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.336       |\n|    n_updates                    | 120         |\n|    policy_gradient_loss         | -0.00188    |\n|    value_loss                   | 0.718       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 68.1        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 2.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0738      |\n|    water_produced               | 0.25        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 8.29         |\n| time/                           |              |\n|    fps                          | 806          |\n|    iterations                   | 42           |\n|    time_elapsed                 | 208          |\n|    total_timesteps              | 168000       |\n| train/                          |              |\n|    approx_kl                    | 0.0055486606 |\n|    clip_fraction                | 0.00817      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.34        |\n|    explained_variance           | 0.000899     |\n|    learning_rate                | 0.001        |\n|    loss                         | 1.64         |\n|    n_updates                    | 123          |\n|    policy_gradient_loss         | -0.000688    |\n|    value_loss                   | 2.39         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 69.9         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 1            |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0727       |\n|    water_produced               | 0            |\n--------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 100        |\n|    ep_rew_mean                  | 8.85       |\n| time/                           |            |\n|    fps                          | 807        |\n|    iterations                   | 43         |\n|    time_elapsed                 | 213        |\n|    total_timesteps              | 172000     |\n| train/                          |            |\n|    approx_kl                    | 0.01063545 |\n|    clip_fraction                | 0.0574     |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -2.25      |\n|    explained_variance           | 0.699      |\n|    learning_rate                | 0.001      |\n|    loss                         | 0.0182     |\n|    n_updates                    | 126        |\n|    policy_gradient_loss         | -0.00102   |\n|    value_loss                   | 0.049      |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 68.7       |\n|    action_queue_updates_total   | 99         |\n|    ice_dug                      | 2          |\n|    unit_deliver_ice_reward      | 0          |\n|    unit_move_to_ice_reward      | 0.0742     |\n|    water_produced               | 0.375      |\n------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 9.55        |\n| time/                           |             |\n|    fps                          | 807         |\n|    iterations                   | 44          |\n|    time_elapsed                 | 217         |\n|    total_timesteps              | 176000      |\n| train/                          |             |\n|    approx_kl                    | 0.007934834 |\n|    clip_fraction                | 0.0167      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.19       |\n|    explained_variance           | 0.0095      |\n|    learning_rate                | 0.001       |\n|    loss                         | 2.16        |\n|    n_updates                    | 129         |\n|    policy_gradient_loss         | -0.0028     |\n|    value_loss                   | 6.26        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 71.3        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 1.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0737      |\n|    water_produced               | 0.25        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 8.6         |\n| time/                           |             |\n|    fps                          | 808         |\n|    iterations                   | 45          |\n|    time_elapsed                 | 222         |\n|    total_timesteps              | 180000      |\n| train/                          |             |\n|    approx_kl                    | 0.020135831 |\n|    clip_fraction                | 0.165       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.04       |\n|    explained_variance           | -0.00566    |\n|    learning_rate                | 0.001       |\n|    loss                         | 1.02        |\n|    n_updates                    | 132         |\n|    policy_gradient_loss         | -0.000435   |\n|    value_loss                   | 2.27        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 72.5        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 5           |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0795      |\n|    water_produced               | 0.125       |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 11.3        |\n| time/                           |             |\n|    fps                          | 808         |\n|    iterations                   | 46          |\n|    time_elapsed                 | 227         |\n|    total_timesteps              | 184000      |\n| train/                          |             |\n|    approx_kl                    | 0.014368674 |\n|    clip_fraction                | 0.0893      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.95       |\n|    explained_variance           | 0.0785      |\n|    learning_rate                | 0.001       |\n|    loss                         | 0.268       |\n|    n_updates                    | 135         |\n|    policy_gradient_loss         | -0.00209    |\n|    value_loss                   | 0.542       |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 77          |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 9.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0815      |\n|    water_produced               | 1.5         |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 14.5        |\n| time/                           |             |\n|    fps                          | 809         |\n|    iterations                   | 47          |\n|    time_elapsed                 | 232         |\n|    total_timesteps              | 188000      |\n| train/                          |             |\n|    approx_kl                    | 0.012214827 |\n|    clip_fraction                | 0.159       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.81       |\n|    explained_variance           | 0.00058     |\n|    learning_rate                | 0.001       |\n|    loss                         | 12.6        |\n|    n_updates                    | 138         |\n|    policy_gradient_loss         | -0.00141    |\n|    value_loss                   | 23.4        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 83.7        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 21          |\n|    unit_deliver_ice_reward      | 0.02        |\n|    unit_move_to_ice_reward      | 0.0742      |\n|    water_produced               | 2.75        |\n-------------------------------------------------\nEval num_timesteps=192000, episode_reward=110.34 +/- 173.80\nEpisode length: 304.00 +/- 6.00\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 304          |\n|    mean_reward                  | 110          |\n| time/                           |              |\n|    total_timesteps              | 192000       |\n| train/                          |              |\n|    approx_kl                    | 0.0015164765 |\n|    clip_fraction                | 8.33e-05     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.74        |\n|    explained_variance           | 0.00954      |\n|    learning_rate                | 0.001        |\n|    loss                         | 6.5          |\n|    n_updates                    | 141          |\n|    policy_gradient_loss         | 0.000842     |\n|    value_loss                   | 14.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 78.6         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 10           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.076        |\n|    water_produced               | 1.12         |\n--------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 100      |\n|    ep_rew_mean     | 14.3     |\n| time/              |          |\n|    fps             | 649      |\n|    iterations      | 48       |\n|    time_elapsed    | 295      |\n|    total_timesteps | 192000   |\n---------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 100        |\n|    ep_rew_mean                  | 13.8       |\n| time/                           |            |\n|    fps                          | 653        |\n|    iterations                   | 49         |\n|    time_elapsed                 | 300        |\n|    total_timesteps              | 196000     |\n| train/                          |            |\n|    approx_kl                    | 0.01638445 |\n|    clip_fraction                | 0.124      |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -1.68      |\n|    explained_variance           | 0.0179     |\n|    learning_rate                | 0.001      |\n|    loss                         | 3.84       |\n|    n_updates                    | 144        |\n|    policy_gradient_loss         | -0.00398   |\n|    value_loss                   | 7.39       |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 81.2       |\n|    action_queue_updates_total   | 99         |\n|    ice_dug                      | 6.5        |\n|    unit_deliver_ice_reward      | 0          |\n|    unit_move_to_ice_reward      | 0.08       |\n|    water_produced               | 0.75       |\n------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 15.2        |\n| time/                           |             |\n|    fps                          | 654         |\n|    iterations                   | 50          |\n|    time_elapsed                 | 305         |\n|    total_timesteps              | 200000      |\n| train/                          |             |\n|    approx_kl                    | 0.008126962 |\n|    clip_fraction                | 0.0621      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.75       |\n|    explained_variance           | 0.0165      |\n|    learning_rate                | 0.001       |\n|    loss                         | 1.8         |\n|    n_updates                    | 147         |\n|    policy_gradient_loss         | -0.00171    |\n|    value_loss                   | 4.24        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 75.5        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 10.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.083       |\n|    water_produced               | 1.75        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 13.4        |\n| time/                           |             |\n|    fps                          | 657         |\n|    iterations                   | 51          |\n|    time_elapsed                 | 310         |\n|    total_timesteps              | 204000      |\n| train/                          |             |\n|    approx_kl                    | 0.010143241 |\n|    clip_fraction                | 0.0326      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.63       |\n|    explained_variance           | -0.0165     |\n|    learning_rate                | 0.001       |\n|    loss                         | 3.87        |\n|    n_updates                    | 150         |\n|    policy_gradient_loss         | 0.000761    |\n|    value_loss                   | 13.1        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 77.1        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 3.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0843      |\n|    water_produced               | 0.375       |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 14.4         |\n| time/                           |              |\n|    fps                          | 660          |\n|    iterations                   | 52           |\n|    time_elapsed                 | 315          |\n|    total_timesteps              | 208000       |\n| train/                          |              |\n|    approx_kl                    | 0.0058030374 |\n|    clip_fraction                | 0.0397       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.6         |\n|    explained_variance           | -0.0519      |\n|    learning_rate                | 0.001        |\n|    loss                         | 2.11         |\n|    n_updates                    | 153          |\n|    policy_gradient_loss         | -0.00194     |\n|    value_loss                   | 3.87         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 72.8         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 5            |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.086        |\n|    water_produced               | 0.875        |\n--------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 100        |\n|    ep_rew_mean                  | 21         |\n| time/                           |            |\n|    fps                          | 662        |\n|    iterations                   | 53         |\n|    time_elapsed                 | 319        |\n|    total_timesteps              | 212000     |\n| train/                          |            |\n|    approx_kl                    | 0.00427904 |\n|    clip_fraction                | 0.0142     |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -1.61      |\n|    explained_variance           | -0.0173    |\n|    learning_rate                | 0.001      |\n|    loss                         | 8.88       |\n|    n_updates                    | 156        |\n|    policy_gradient_loss         | -0.00103   |\n|    value_loss                   | 20         |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 75.4       |\n|    action_queue_updates_total   | 99         |\n|    ice_dug                      | 20.5       |\n|    unit_deliver_ice_reward      | 0          |\n|    unit_move_to_ice_reward      | 0.0855     |\n|    water_produced               | 3.75       |\n------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 26.1        |\n| time/                           |             |\n|    fps                          | 665         |\n|    iterations                   | 54          |\n|    time_elapsed                 | 324         |\n|    total_timesteps              | 216000      |\n| train/                          |             |\n|    approx_kl                    | 0.013215785 |\n|    clip_fraction                | 0.154       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.66       |\n|    explained_variance           | 0.000485    |\n|    learning_rate                | 0.001       |\n|    loss                         | 22.5        |\n|    n_updates                    | 159         |\n|    policy_gradient_loss         | 0.00171     |\n|    value_loss                   | 47.1        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 84.8        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 39.5        |\n|    unit_deliver_ice_reward      | 0.013       |\n|    unit_move_to_ice_reward      | 0.081       |\n|    water_produced               | 5.88        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 29.1         |\n| time/                           |              |\n|    fps                          | 667          |\n|    iterations                   | 55           |\n|    time_elapsed                 | 329          |\n|    total_timesteps              | 220000       |\n| train/                          |              |\n|    approx_kl                    | 0.0056456276 |\n|    clip_fraction                | 8.33e-05     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.61        |\n|    explained_variance           | 0.00495      |\n|    learning_rate                | 0.001        |\n|    loss                         | 22           |\n|    n_updates                    | 162          |\n|    policy_gradient_loss         | -0.000948    |\n|    value_loss                   | 62           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 85.9         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 28.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.085        |\n|    water_produced               | 6.13         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 24.3         |\n| time/                           |              |\n|    fps                          | 668          |\n|    iterations                   | 56           |\n|    time_elapsed                 | 335          |\n|    total_timesteps              | 224000       |\n| train/                          |              |\n|    approx_kl                    | 0.0026866416 |\n|    clip_fraction                | 0.00125      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.6         |\n|    explained_variance           | 0.00137      |\n|    learning_rate                | 0.001        |\n|    loss                         | 84           |\n|    n_updates                    | 165          |\n|    policy_gradient_loss         | 3.04e-05     |\n|    value_loss                   | 133          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 82.9         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 14.5         |\n|    unit_deliver_ice_reward      | 0.00675      |\n|    unit_move_to_ice_reward      | 0.081        |\n|    water_produced               | 2.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 16.5         |\n| time/                           |              |\n|    fps                          | 671          |\n|    iterations                   | 57           |\n|    time_elapsed                 | 339          |\n|    total_timesteps              | 228000       |\n| train/                          |              |\n|    approx_kl                    | 0.0047021685 |\n|    clip_fraction                | 0.0106       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.52        |\n|    explained_variance           | 0.00939      |\n|    learning_rate                | 0.001        |\n|    loss                         | 11.5         |\n|    n_updates                    | 168          |\n|    policy_gradient_loss         | -0.000276    |\n|    value_loss                   | 23.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 82.7         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 15.5         |\n|    unit_deliver_ice_reward      | 0.0065       |\n|    unit_move_to_ice_reward      | 0.082        |\n|    water_produced               | 2.25         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 17.6         |\n| time/                           |              |\n|    fps                          | 673          |\n|    iterations                   | 58           |\n|    time_elapsed                 | 344          |\n|    total_timesteps              | 232000       |\n| train/                          |              |\n|    approx_kl                    | 0.0065783784 |\n|    clip_fraction                | 0.0606       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.00422      |\n|    learning_rate                | 0.001        |\n|    loss                         | 4.9          |\n|    n_updates                    | 171          |\n|    policy_gradient_loss         | 0.00198      |\n|    value_loss                   | 10.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 80.9         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 8.5          |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0863       |\n|    water_produced               | 2.12         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 19.8        |\n| time/                           |             |\n|    fps                          | 675         |\n|    iterations                   | 59          |\n|    time_elapsed                 | 349         |\n|    total_timesteps              | 236000      |\n| train/                          |             |\n|    approx_kl                    | 0.004138629 |\n|    clip_fraction                | 0.0192      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.65       |\n|    explained_variance           | 0.0331      |\n|    learning_rate                | 0.001       |\n|    loss                         | 7.99        |\n|    n_updates                    | 174         |\n|    policy_gradient_loss         | 0.000635    |\n|    value_loss                   | 22.2        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 82.5        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 20.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0852      |\n|    water_produced               | 4.5         |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 19.6         |\n| time/                           |              |\n|    fps                          | 677          |\n|    iterations                   | 60           |\n|    time_elapsed                 | 353          |\n|    total_timesteps              | 240000       |\n| train/                          |              |\n|    approx_kl                    | 0.0021422873 |\n|    clip_fraction                | 0.00625      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.71        |\n|    explained_variance           | 0.0473       |\n|    learning_rate                | 0.001        |\n|    loss                         | 21.8         |\n|    n_updates                    | 177          |\n|    policy_gradient_loss         | 0.000486     |\n|    value_loss                   | 39.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 80.3         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 15           |\n|    unit_deliver_ice_reward      | 0.00675      |\n|    unit_move_to_ice_reward      | 0.0845       |\n|    water_produced               | 2.88         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 27.6         |\n| time/                           |              |\n|    fps                          | 680          |\n|    iterations                   | 61           |\n|    time_elapsed                 | 358          |\n|    total_timesteps              | 244000       |\n| train/                          |              |\n|    approx_kl                    | 0.0020614949 |\n|    clip_fraction                | 0.00333      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.81        |\n|    explained_variance           | 0.0129       |\n|    learning_rate                | 0.001        |\n|    loss                         | 8.48         |\n|    n_updates                    | 180          |\n|    policy_gradient_loss         | -0.000443    |\n|    value_loss                   | 16.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 83.3         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 27.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0807       |\n|    water_produced               | 6            |\n--------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 100        |\n|    ep_rew_mean                  | 30.9       |\n| time/                           |            |\n|    fps                          | 682        |\n|    iterations                   | 62         |\n|    time_elapsed                 | 363        |\n|    total_timesteps              | 248000     |\n| train/                          |            |\n|    approx_kl                    | 0.03419327 |\n|    clip_fraction                | 0.304      |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -1.56      |\n|    explained_variance           | 0.0262     |\n|    learning_rate                | 0.001      |\n|    loss                         | 58         |\n|    n_updates                    | 183        |\n|    policy_gradient_loss         | 0.00541    |\n|    value_loss                   | 153        |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 82.2       |\n|    action_queue_updates_total   | 99         |\n|    ice_dug                      | 26.5       |\n|    unit_deliver_ice_reward      | 0          |\n|    unit_move_to_ice_reward      | 0.0853     |\n|    water_produced               | 4.38       |\n------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 31.5        |\n| time/                           |             |\n|    fps                          | 682         |\n|    iterations                   | 63          |\n|    time_elapsed                 | 368         |\n|    total_timesteps              | 252000      |\n| train/                          |             |\n|    approx_kl                    | 0.016869346 |\n|    clip_fraction                | 0.0541      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.51       |\n|    explained_variance           | 0.000959    |\n|    learning_rate                | 0.001       |\n|    loss                         | 42.2        |\n|    n_updates                    | 186         |\n|    policy_gradient_loss         | -7.59e-05   |\n|    value_loss                   | 90.4        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 83.4        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 36.5        |\n|    unit_deliver_ice_reward      | 0.00675     |\n|    unit_move_to_ice_reward      | 0.0835      |\n|    water_produced               | 8           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 30.1        |\n| time/                           |             |\n|    fps                          | 685         |\n|    iterations                   | 64          |\n|    time_elapsed                 | 373         |\n|    total_timesteps              | 256000      |\n| train/                          |             |\n|    approx_kl                    | 0.008310345 |\n|    clip_fraction                | 0.0382      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.52       |\n|    explained_variance           | 0.0359      |\n|    learning_rate                | 0.001       |\n|    loss                         | 53.6        |\n|    n_updates                    | 189         |\n|    policy_gradient_loss         | -0.00117    |\n|    value_loss                   | 108         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 83.3        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 17          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0822      |\n|    water_produced               | 1.5         |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 31.6        |\n| time/                           |             |\n|    fps                          | 687         |\n|    iterations                   | 65          |\n|    time_elapsed                 | 378         |\n|    total_timesteps              | 260000      |\n| train/                          |             |\n|    approx_kl                    | 0.016206682 |\n|    clip_fraction                | 0.098       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.58       |\n|    explained_variance           | 0.0368      |\n|    learning_rate                | 0.001       |\n|    loss                         | 23.1        |\n|    n_updates                    | 192         |\n|    policy_gradient_loss         | 0.00221     |\n|    value_loss                   | 48.8        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 79          |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 40          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0875      |\n|    water_produced               | 6.88        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 36.9         |\n| time/                           |              |\n|    fps                          | 688          |\n|    iterations                   | 66           |\n|    time_elapsed                 | 383          |\n|    total_timesteps              | 264000       |\n| train/                          |              |\n|    approx_kl                    | 0.0101397475 |\n|    clip_fraction                | 0.0979       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.53        |\n|    explained_variance           | 0.0428       |\n|    learning_rate                | 0.001        |\n|    loss                         | 62.6         |\n|    n_updates                    | 195          |\n|    policy_gradient_loss         | 0.00271      |\n|    value_loss                   | 115          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 81.5         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 52.5         |\n|    unit_deliver_ice_reward      | 0.0133       |\n|    unit_move_to_ice_reward      | 0.0783       |\n|    water_produced               | 7.75         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 36.5        |\n| time/                           |             |\n|    fps                          | 691         |\n|    iterations                   | 67          |\n|    time_elapsed                 | 387         |\n|    total_timesteps              | 268000      |\n| train/                          |             |\n|    approx_kl                    | 0.005129649 |\n|    clip_fraction                | 0.0352      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.62       |\n|    explained_variance           | 0.0463      |\n|    learning_rate                | 0.001       |\n|    loss                         | 60.6        |\n|    n_updates                    | 198         |\n|    policy_gradient_loss         | -0.00134    |\n|    value_loss                   | 118         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 79.3        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 25          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0845      |\n|    water_produced               | 5.38        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 28.8         |\n| time/                           |              |\n|    fps                          | 692          |\n|    iterations                   | 68           |\n|    time_elapsed                 | 392          |\n|    total_timesteps              | 272000       |\n| train/                          |              |\n|    approx_kl                    | 0.0040717255 |\n|    clip_fraction                | 0.00283      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.71        |\n|    explained_variance           | 0.0273       |\n|    learning_rate                | 0.001        |\n|    loss                         | 42.6         |\n|    n_updates                    | 201          |\n|    policy_gradient_loss         | -0.00113     |\n|    value_loss                   | 88.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 78.5         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 23           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.083        |\n|    water_produced               | 3.13         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 25.5        |\n| time/                           |             |\n|    fps                          | 693         |\n|    iterations                   | 69          |\n|    time_elapsed                 | 397         |\n|    total_timesteps              | 276000      |\n| train/                          |             |\n|    approx_kl                    | 0.004534087 |\n|    clip_fraction                | 0.0439      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.74       |\n|    explained_variance           | 0.13        |\n|    learning_rate                | 0.001       |\n|    loss                         | 16.4        |\n|    n_updates                    | 204         |\n|    policy_gradient_loss         | -0.00169    |\n|    value_loss                   | 37.5        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 73          |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 21          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0832      |\n|    water_produced               | 1.75        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 18.6        |\n| time/                           |             |\n|    fps                          | 694         |\n|    iterations                   | 70          |\n|    time_elapsed                 | 402         |\n|    total_timesteps              | 280000      |\n| train/                          |             |\n|    approx_kl                    | 0.004360068 |\n|    clip_fraction                | 0.0304      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.73       |\n|    explained_variance           | 0.0277      |\n|    learning_rate                | 0.001       |\n|    loss                         | 27.7        |\n|    n_updates                    | 207         |\n|    policy_gradient_loss         | -0.000134   |\n|    value_loss                   | 56          |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 71.7        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 22          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.083       |\n|    water_produced               | 1.75        |\n-------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 100        |\n|    ep_rew_mean                  | 28.1       |\n| time/                           |            |\n|    fps                          | 696        |\n|    iterations                   | 71         |\n|    time_elapsed                 | 407        |\n|    total_timesteps              | 284000     |\n| train/                          |            |\n|    approx_kl                    | 0.00420167 |\n|    clip_fraction                | 0.0085     |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -1.74      |\n|    explained_variance           | 0.111      |\n|    learning_rate                | 0.001      |\n|    loss                         | 14.8       |\n|    n_updates                    | 210        |\n|    policy_gradient_loss         | 1.18e-05   |\n|    value_loss                   | 25.6       |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 71         |\n|    action_queue_updates_total   | 99         |\n|    ice_dug                      | 29         |\n|    unit_deliver_ice_reward      | 0          |\n|    unit_move_to_ice_reward      | 0.085      |\n|    water_produced               | 4.25       |\n------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 33.4        |\n| time/                           |             |\n|    fps                          | 697         |\n|    iterations                   | 72          |\n|    time_elapsed                 | 412         |\n|    total_timesteps              | 288000      |\n| train/                          |             |\n|    approx_kl                    | 0.015085803 |\n|    clip_fraction                | 0.147       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.64       |\n|    explained_variance           | 0.0372      |\n|    learning_rate                | 0.001       |\n|    loss                         | 83.2        |\n|    n_updates                    | 213         |\n|    policy_gradient_loss         | -0.000385   |\n|    value_loss                   | 246         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 71.7        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 35.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0863      |\n|    water_produced               | 6.25        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 33.7         |\n| time/                           |              |\n|    fps                          | 699          |\n|    iterations                   | 73           |\n|    time_elapsed                 | 417          |\n|    total_timesteps              | 292000       |\n| train/                          |              |\n|    approx_kl                    | 0.0007377807 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.67        |\n|    explained_variance           | 0.0879       |\n|    learning_rate                | 0.001        |\n|    loss                         | 49.9         |\n|    n_updates                    | 216          |\n|    policy_gradient_loss         | -0.000887    |\n|    value_loss                   | 112          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 71.3         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 33           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0888       |\n|    water_produced               | 4.12         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 34.6         |\n| time/                           |              |\n|    fps                          | 700          |\n|    iterations                   | 74           |\n|    time_elapsed                 | 422          |\n|    total_timesteps              | 296000       |\n| train/                          |              |\n|    approx_kl                    | 0.0051158518 |\n|    clip_fraction                | 0.0191       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.73        |\n|    explained_variance           | 0.0737       |\n|    learning_rate                | 0.001        |\n|    loss                         | 30.8         |\n|    n_updates                    | 219          |\n|    policy_gradient_loss         | -0.00155     |\n|    value_loss                   | 71.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 73.3         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 31.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0882       |\n|    water_produced               | 2.63         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 28.1        |\n| time/                           |             |\n|    fps                          | 702         |\n|    iterations                   | 75          |\n|    time_elapsed                 | 427         |\n|    total_timesteps              | 300000      |\n| train/                          |             |\n|    approx_kl                    | 0.013807791 |\n|    clip_fraction                | 0.1         |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.74       |\n|    explained_variance           | 0.107       |\n|    learning_rate                | 0.001       |\n|    loss                         | 91.6        |\n|    n_updates                    | 222         |\n|    policy_gradient_loss         | -0.000435   |\n|    value_loss                   | 129         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 70.4        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 8.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0862      |\n|    water_produced               | 1.75        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 26           |\n| time/                           |              |\n|    fps                          | 702          |\n|    iterations                   | 76           |\n|    time_elapsed                 | 432          |\n|    total_timesteps              | 304000       |\n| train/                          |              |\n|    approx_kl                    | 0.0024528014 |\n|    clip_fraction                | 0.00742      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.83        |\n|    explained_variance           | 0.0829       |\n|    learning_rate                | 0.001        |\n|    loss                         | 14.1         |\n|    n_updates                    | 225          |\n|    policy_gradient_loss         | -0.000532    |\n|    value_loss                   | 38.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 65.6         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 18           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0843       |\n|    water_produced               | 2.63         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 36.4         |\n| time/                           |              |\n|    fps                          | 704          |\n|    iterations                   | 77           |\n|    time_elapsed                 | 437          |\n|    total_timesteps              | 308000       |\n| train/                          |              |\n|    approx_kl                    | 0.0026965921 |\n|    clip_fraction                | 0.01         |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.68        |\n|    explained_variance           | 0.0409       |\n|    learning_rate                | 0.001        |\n|    loss                         | 66.8         |\n|    n_updates                    | 228          |\n|    policy_gradient_loss         | -0.000947    |\n|    value_loss                   | 100          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 65.6         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 28.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0873       |\n|    water_produced               | 6.37         |\n--------------------------------------------------\n-----------------------------------------------\n| rollout/                        |           |\n|    ep_len_mean                  | 100       |\n|    ep_rew_mean                  | 40        |\n| time/                           |           |\n|    fps                          | 705       |\n|    iterations                   | 78        |\n|    time_elapsed                 | 442       |\n|    total_timesteps              | 312000    |\n| train/                          |           |\n|    approx_kl                    | 0.0191645 |\n|    clip_fraction                | 0.178     |\n|    clip_range                   | 0.2       |\n|    entropy_loss                 | -1.55     |\n|    explained_variance           | -0.0268   |\n|    learning_rate                | 0.001     |\n|    loss                         | 46.9      |\n|    n_updates                    | 231       |\n|    policy_gradient_loss         | 0.00216   |\n|    value_loss                   | 97.8      |\n| train_metrics/                  |           |\n|    action_queue_updates_success | 67.4      |\n|    action_queue_updates_total   | 99        |\n|    ice_dug                      | 17        |\n|    unit_deliver_ice_reward      | 0         |\n|    unit_move_to_ice_reward      | 0.0822    |\n|    water_produced               | 3.12      |\n-----------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 31.5        |\n| time/                           |             |\n|    fps                          | 706         |\n|    iterations                   | 79          |\n|    time_elapsed                 | 447         |\n|    total_timesteps              | 316000      |\n| train/                          |             |\n|    approx_kl                    | 0.011459069 |\n|    clip_fraction                | 0.044       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.58       |\n|    explained_variance           | -0.00486    |\n|    learning_rate                | 0.001       |\n|    loss                         | 26.4        |\n|    n_updates                    | 234         |\n|    policy_gradient_loss         | -0.0018     |\n|    value_loss                   | 53.1        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 67.2        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 15.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0807      |\n|    water_produced               | 2           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 21.6        |\n| time/                           |             |\n|    fps                          | 708         |\n|    iterations                   | 80          |\n|    time_elapsed                 | 451         |\n|    total_timesteps              | 320000      |\n| train/                          |             |\n|    approx_kl                    | 0.013517143 |\n|    clip_fraction                | 0.0309      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.61       |\n|    explained_variance           | 0.137       |\n|    learning_rate                | 0.001       |\n|    loss                         | 14.2        |\n|    n_updates                    | 237         |\n|    policy_gradient_loss         | -0.00113    |\n|    value_loss                   | 32.9        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 66.5        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 10          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0788      |\n|    water_produced               | 2.12        |\n-------------------------------------------------\n-----------------------------------------------\n| rollout/                        |           |\n|    ep_len_mean                  | 100       |\n|    ep_rew_mean                  | 23.6      |\n| time/                           |           |\n|    fps                          | 709       |\n|    iterations                   | 81        |\n|    time_elapsed                 | 456       |\n|    total_timesteps              | 324000    |\n| train/                          |           |\n|    approx_kl                    | 0.0027653 |\n|    clip_fraction                | 0.00192   |\n|    clip_range                   | 0.2       |\n|    entropy_loss                 | -1.66     |\n|    explained_variance           | 0.0321    |\n|    learning_rate                | 0.001     |\n|    loss                         | 11.9      |\n|    n_updates                    | 240       |\n|    policy_gradient_loss         | 0.000344  |\n|    value_loss                   | 30.4      |\n| train_metrics/                  |           |\n|    action_queue_updates_success | 68.5      |\n|    action_queue_updates_total   | 99        |\n|    ice_dug                      | 13.5      |\n|    unit_deliver_ice_reward      | 0         |\n|    unit_move_to_ice_reward      | 0.0805    |\n|    water_produced               | 2.75      |\n-----------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 26.7         |\n| time/                           |              |\n|    fps                          | 709          |\n|    iterations                   | 82           |\n|    time_elapsed                 | 462          |\n|    total_timesteps              | 328000       |\n| train/                          |              |\n|    approx_kl                    | 0.0047942293 |\n|    clip_fraction                | 0.0267       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.71        |\n|    explained_variance           | 0.195        |\n|    learning_rate                | 0.001        |\n|    loss                         | 14.9         |\n|    n_updates                    | 243          |\n|    policy_gradient_loss         | -0.000748    |\n|    value_loss                   | 34.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 70.8         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 16.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0828       |\n|    water_produced               | 3.12         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 22.5         |\n| time/                           |              |\n|    fps                          | 710          |\n|    iterations                   | 83           |\n|    time_elapsed                 | 467          |\n|    total_timesteps              | 332000       |\n| train/                          |              |\n|    approx_kl                    | 0.0047585224 |\n|    clip_fraction                | 0.0195       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.72        |\n|    explained_variance           | 0.122        |\n|    learning_rate                | 0.001        |\n|    loss                         | 25           |\n|    n_updates                    | 246          |\n|    policy_gradient_loss         | -0.000975    |\n|    value_loss                   | 58.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 76.7         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 17.5         |\n|    unit_deliver_ice_reward      | 0.00675      |\n|    unit_move_to_ice_reward      | 0.0797       |\n|    water_produced               | 2.75         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 15.6        |\n| time/                           |             |\n|    fps                          | 712         |\n|    iterations                   | 84          |\n|    time_elapsed                 | 471         |\n|    total_timesteps              | 336000      |\n| train/                          |             |\n|    approx_kl                    | 0.008297207 |\n|    clip_fraction                | 0.0626      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.82       |\n|    explained_variance           | 0.315       |\n|    learning_rate                | 0.001       |\n|    loss                         | 13.7        |\n|    n_updates                    | 249         |\n|    policy_gradient_loss         | -9.28e-05   |\n|    value_loss                   | 19.8        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 71.1        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 4           |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0737      |\n|    water_produced               | 0.625       |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 13.5         |\n| time/                           |              |\n|    fps                          | 713          |\n|    iterations                   | 85           |\n|    time_elapsed                 | 476          |\n|    total_timesteps              | 340000       |\n| train/                          |              |\n|    approx_kl                    | 0.0053018024 |\n|    clip_fraction                | 0.0142       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.85        |\n|    explained_variance           | 0.307        |\n|    learning_rate                | 0.001        |\n|    loss                         | 2.43         |\n|    n_updates                    | 252          |\n|    policy_gradient_loss         | -0.000382    |\n|    value_loss                   | 9.09         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 70.7         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 6            |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0793       |\n|    water_produced               | 0.625        |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 14.4        |\n| time/                           |             |\n|    fps                          | 714         |\n|    iterations                   | 86          |\n|    time_elapsed                 | 481         |\n|    total_timesteps              | 344000      |\n| train/                          |             |\n|    approx_kl                    | 0.009398857 |\n|    clip_fraction                | 0.0486      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.93       |\n|    explained_variance           | 0.216       |\n|    learning_rate                | 0.001       |\n|    loss                         | 4.01        |\n|    n_updates                    | 255         |\n|    policy_gradient_loss         | -0.00221    |\n|    value_loss                   | 9.67        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 67.8        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 9.5         |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.077       |\n|    water_produced               | 1.37        |\n-------------------------------------------------\n-----------------------------------------------\n| rollout/                        |           |\n|    ep_len_mean                  | 100       |\n|    ep_rew_mean                  | 15        |\n| time/                           |           |\n|    fps                          | 715       |\n|    iterations                   | 87        |\n|    time_elapsed                 | 486       |\n|    total_timesteps              | 348000    |\n| train/                          |           |\n|    approx_kl                    | 0.0046793 |\n|    clip_fraction                | 0.0168    |\n|    clip_range                   | 0.2       |\n|    entropy_loss                 | -1.82     |\n|    explained_variance           | 0.12      |\n|    learning_rate                | 0.001     |\n|    loss                         | 10.4      |\n|    n_updates                    | 258       |\n|    policy_gradient_loss         | -0.00113  |\n|    value_loss                   | 23.3      |\n| train_metrics/                  |           |\n|    action_queue_updates_success | 66.6      |\n|    action_queue_updates_total   | 99        |\n|    ice_dug                      | 11        |\n|    unit_deliver_ice_reward      | 0         |\n|    unit_move_to_ice_reward      | 0.0752    |\n|    water_produced               | 0.875     |\n-----------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 17.5        |\n| time/                           |             |\n|    fps                          | 716         |\n|    iterations                   | 88          |\n|    time_elapsed                 | 491         |\n|    total_timesteps              | 352000      |\n| train/                          |             |\n|    approx_kl                    | 0.016615387 |\n|    clip_fraction                | 0.113       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.83       |\n|    explained_variance           | 0.225       |\n|    learning_rate                | 0.001       |\n|    loss                         | 5.98        |\n|    n_updates                    | 261         |\n|    policy_gradient_loss         | -0.00232    |\n|    value_loss                   | 12.7        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 65.2        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 14          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0758      |\n|    water_produced               | 1.12        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 19.2        |\n| time/                           |             |\n|    fps                          | 717         |\n|    iterations                   | 89          |\n|    time_elapsed                 | 496         |\n|    total_timesteps              | 356000      |\n| train/                          |             |\n|    approx_kl                    | 0.010565731 |\n|    clip_fraction                | 0.0642      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.94       |\n|    explained_variance           | 0.0662      |\n|    learning_rate                | 0.001       |\n|    loss                         | 10.6        |\n|    n_updates                    | 264         |\n|    policy_gradient_loss         | -0.00217    |\n|    value_loss                   | 23.8        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 64.8        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 26          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0718      |\n|    water_produced               | 2.38        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 21.4        |\n| time/                           |             |\n|    fps                          | 718         |\n|    iterations                   | 90          |\n|    time_elapsed                 | 501         |\n|    total_timesteps              | 360000      |\n| train/                          |             |\n|    approx_kl                    | 0.014021452 |\n|    clip_fraction                | 0.126       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.92       |\n|    explained_variance           | 0.152       |\n|    learning_rate                | 0.001       |\n|    loss                         | 39.3        |\n|    n_updates                    | 267         |\n|    policy_gradient_loss         | -0.00215    |\n|    value_loss                   | 97.3        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 63.9        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 16          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0783      |\n|    water_produced               | 2.37        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 24.7         |\n| time/                           |              |\n|    fps                          | 719          |\n|    iterations                   | 91           |\n|    time_elapsed                 | 505          |\n|    total_timesteps              | 364000       |\n| train/                          |              |\n|    approx_kl                    | 0.0056929947 |\n|    clip_fraction                | 0.022        |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.93        |\n|    explained_variance           | 0.137        |\n|    learning_rate                | 0.001        |\n|    loss                         | 26           |\n|    n_updates                    | 270          |\n|    policy_gradient_loss         | -0.00187     |\n|    value_loss                   | 50.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 68.1         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 9.5          |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0775       |\n|    water_produced               | 2.12         |\n--------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 100        |\n|    ep_rew_mean                  | 21.8       |\n| time/                           |            |\n|    fps                          | 720        |\n|    iterations                   | 92         |\n|    time_elapsed                 | 510        |\n|    total_timesteps              | 368000     |\n| train/                          |            |\n|    approx_kl                    | 0.00579908 |\n|    clip_fraction                | 0.0143     |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -1.98      |\n|    explained_variance           | 0.266      |\n|    learning_rate                | 0.001      |\n|    loss                         | 39         |\n|    n_updates                    | 273        |\n|    policy_gradient_loss         | -0.0011    |\n|    value_loss                   | 61.2       |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 69.5       |\n|    action_queue_updates_total   | 99         |\n|    ice_dug                      | 12.5       |\n|    unit_deliver_ice_reward      | 0          |\n|    unit_move_to_ice_reward      | 0.0803     |\n|    water_produced               | 1.88       |\n------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 24.2        |\n| time/                           |             |\n|    fps                          | 721         |\n|    iterations                   | 93          |\n|    time_elapsed                 | 515         |\n|    total_timesteps              | 372000      |\n| train/                          |             |\n|    approx_kl                    | 0.009402441 |\n|    clip_fraction                | 0.046       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.83       |\n|    explained_variance           | 0.0822      |\n|    learning_rate                | 0.001       |\n|    loss                         | 21.1        |\n|    n_updates                    | 276         |\n|    policy_gradient_loss         | -0.00197    |\n|    value_loss                   | 45.7        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 68.6        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 24          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0788      |\n|    water_produced               | 2.5         |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 23.5        |\n| time/                           |             |\n|    fps                          | 723         |\n|    iterations                   | 94          |\n|    time_elapsed                 | 520         |\n|    total_timesteps              | 376000      |\n| train/                          |             |\n|    approx_kl                    | 0.001594777 |\n|    clip_fraction                | 0.00225     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.85       |\n|    explained_variance           | 0.379       |\n|    learning_rate                | 0.001       |\n|    loss                         | 24.4        |\n|    n_updates                    | 279         |\n|    policy_gradient_loss         | -0.000893   |\n|    value_loss                   | 46.1        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 71.1        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 20.5        |\n|    unit_deliver_ice_reward      | 0.0065      |\n|    unit_move_to_ice_reward      | 0.0795      |\n|    water_produced               | 2.75        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 24.9         |\n| time/                           |              |\n|    fps                          | 723          |\n|    iterations                   | 95           |\n|    time_elapsed                 | 525          |\n|    total_timesteps              | 380000       |\n| train/                          |              |\n|    approx_kl                    | 0.0037766874 |\n|    clip_fraction                | 0.0135       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.78        |\n|    explained_variance           | 0.193        |\n|    learning_rate                | 0.001        |\n|    loss                         | 13.8         |\n|    n_updates                    | 282          |\n|    policy_gradient_loss         | -0.000454    |\n|    value_loss                   | 31.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 68.2         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 20.5         |\n|    unit_deliver_ice_reward      | 0.00675      |\n|    unit_move_to_ice_reward      | 0.0795       |\n|    water_produced               | 2.5          |\n--------------------------------------------------\nEval num_timesteps=384000, episode_reward=761.04 +/- 1473.72\nEpisode length: 322.00 +/- 42.00\n-------------------------------------------------\n| eval/                           |             |\n|    mean_ep_length               | 322         |\n|    mean_reward                  | 761         |\n| time/                           |             |\n|    total_timesteps              | 384000      |\n| train/                          |             |\n|    approx_kl                    | 0.004727203 |\n|    clip_fraction                | 0.0182      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.8        |\n|    explained_variance           | 0.227       |\n|    learning_rate                | 0.001       |\n|    loss                         | 47.6        |\n|    n_updates                    | 285         |\n|    policy_gradient_loss         | -0.000221   |\n|    value_loss                   | 62.5        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 69.9        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 37          |\n|    unit_deliver_ice_reward      | 0.007       |\n|    unit_move_to_ice_reward      | 0.0793      |\n|    water_produced               | 7.37        |\n-------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 100      |\n|    ep_rew_mean     | 36.6     |\n| time/              |          |\n|    fps             | 640      |\n|    iterations      | 96       |\n|    time_elapsed    | 599      |\n|    total_timesteps | 384000   |\n---------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 33.2        |\n| time/                           |             |\n|    fps                          | 642         |\n|    iterations                   | 97          |\n|    time_elapsed                 | 603         |\n|    total_timesteps              | 388000      |\n| train/                          |             |\n|    approx_kl                    | 0.033658482 |\n|    clip_fraction                | 0.267       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.56       |\n|    explained_variance           | 0.176       |\n|    learning_rate                | 0.001       |\n|    loss                         | 86.9        |\n|    n_updates                    | 288         |\n|    policy_gradient_loss         | 0.00294     |\n|    value_loss                   | 161         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 71.2        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 33          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0798      |\n|    water_produced               | 4.5         |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 47.2         |\n| time/                           |              |\n|    fps                          | 644          |\n|    iterations                   | 98           |\n|    time_elapsed                 | 608          |\n|    total_timesteps              | 392000       |\n| train/                          |              |\n|    approx_kl                    | 0.0057491036 |\n|    clip_fraction                | 0.0654       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.61        |\n|    explained_variance           | 0.134        |\n|    learning_rate                | 0.001        |\n|    loss                         | 66.5         |\n|    n_updates                    | 291          |\n|    policy_gradient_loss         | 0.000131     |\n|    value_loss                   | 110          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 71.7         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 59.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0835       |\n|    water_produced               | 8.62         |\n--------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 100        |\n|    ep_rew_mean                  | 45.2       |\n| time/                           |            |\n|    fps                          | 645        |\n|    iterations                   | 99         |\n|    time_elapsed                 | 613        |\n|    total_timesteps              | 396000     |\n| train/                          |            |\n|    approx_kl                    | 0.03343398 |\n|    clip_fraction                | 0.2        |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -1.35      |\n|    explained_variance           | 0.0322     |\n|    learning_rate                | 0.001      |\n|    loss                         | 125        |\n|    n_updates                    | 294        |\n|    policy_gradient_loss         | 0.00989    |\n|    value_loss                   | 260        |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 75.2       |\n|    action_queue_updates_total   | 99         |\n|    ice_dug                      | 46.5       |\n|    unit_deliver_ice_reward      | 0.00675    |\n|    unit_move_to_ice_reward      | 0.087      |\n|    water_produced               | 8.75       |\n------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 51.2        |\n| time/                           |             |\n|    fps                          | 646         |\n|    iterations                   | 100         |\n|    time_elapsed                 | 618         |\n|    total_timesteps              | 400000      |\n| train/                          |             |\n|    approx_kl                    | 0.009904953 |\n|    clip_fraction                | 0.0502      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.26       |\n|    explained_variance           | 0.0737      |\n|    learning_rate                | 0.001       |\n|    loss                         | 64.8        |\n|    n_updates                    | 297         |\n|    policy_gradient_loss         | -0.000954   |\n|    value_loss                   | 128         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 77.1        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 51.5        |\n|    unit_deliver_ice_reward      | 0.0202      |\n|    unit_move_to_ice_reward      | 0.0785      |\n|    water_produced               | 8.5         |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 47.8        |\n| time/                           |             |\n|    fps                          | 647         |\n|    iterations                   | 101         |\n|    time_elapsed                 | 623         |\n|    total_timesteps              | 404000      |\n| train/                          |             |\n|    approx_kl                    | 0.008794735 |\n|    clip_fraction                | 0.022       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.24       |\n|    explained_variance           | 0.0175      |\n|    learning_rate                | 0.001       |\n|    loss                         | 96.3        |\n|    n_updates                    | 300         |\n|    policy_gradient_loss         | -0.00319    |\n|    value_loss                   | 149         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 71.8        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 31.5        |\n|    unit_deliver_ice_reward      | 0.0135      |\n|    unit_move_to_ice_reward      | 0.0853      |\n|    water_produced               | 4.25        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 48.5        |\n| time/                           |             |\n|    fps                          | 649         |\n|    iterations                   | 102         |\n|    time_elapsed                 | 628         |\n|    total_timesteps              | 408000      |\n| train/                          |             |\n|    approx_kl                    | 0.008958035 |\n|    clip_fraction                | 0.0561      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.1        |\n|    explained_variance           | 0.0489      |\n|    learning_rate                | 0.001       |\n|    loss                         | 47.2        |\n|    n_updates                    | 303         |\n|    policy_gradient_loss         | -0.00206    |\n|    value_loss                   | 95.3        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 75          |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 47.5        |\n|    unit_deliver_ice_reward      | 0.0115      |\n|    unit_move_to_ice_reward      | 0.0842      |\n|    water_produced               | 8           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 47.4         |\n| time/                           |              |\n|    fps                          | 650          |\n|    iterations                   | 103          |\n|    time_elapsed                 | 633          |\n|    total_timesteps              | 412000       |\n| train/                          |              |\n|    approx_kl                    | 0.0071340017 |\n|    clip_fraction                | 0.0412       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.14        |\n|    explained_variance           | 0.0674       |\n|    learning_rate                | 0.001        |\n|    loss                         | 158          |\n|    n_updates                    | 306          |\n|    policy_gradient_loss         | 0.00125      |\n|    value_loss                   | 250          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 76.3         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 31           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0883       |\n|    water_produced               | 6.13         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 42.6        |\n| time/                           |             |\n|    fps                          | 651         |\n|    iterations                   | 104         |\n|    time_elapsed                 | 638         |\n|    total_timesteps              | 416000      |\n| train/                          |             |\n|    approx_kl                    | 0.013495232 |\n|    clip_fraction                | 0.0678      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.19       |\n|    explained_variance           | 0.0747      |\n|    learning_rate                | 0.001       |\n|    loss                         | 34.4        |\n|    n_updates                    | 309         |\n|    policy_gradient_loss         | 0.00288     |\n|    value_loss                   | 89.5        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 71.7        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 35.5        |\n|    unit_deliver_ice_reward      | 0.00675     |\n|    unit_move_to_ice_reward      | 0.0833      |\n|    water_produced               | 4.88        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 43.7         |\n| time/                           |              |\n|    fps                          | 652          |\n|    iterations                   | 105          |\n|    time_elapsed                 | 643          |\n|    total_timesteps              | 420000       |\n| train/                          |              |\n|    approx_kl                    | 0.0040433942 |\n|    clip_fraction                | 0.00642      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.15        |\n|    explained_variance           | 0.0826       |\n|    learning_rate                | 0.001        |\n|    loss                         | 100          |\n|    n_updates                    | 312          |\n|    policy_gradient_loss         | -0.00111     |\n|    value_loss                   | 200          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 73.1         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 63           |\n|    unit_deliver_ice_reward      | 0.0133       |\n|    unit_move_to_ice_reward      | 0.0808       |\n|    water_produced               | 13           |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 45.7        |\n| time/                           |             |\n|    fps                          | 654         |\n|    iterations                   | 106         |\n|    time_elapsed                 | 648         |\n|    total_timesteps              | 424000      |\n| train/                          |             |\n|    approx_kl                    | 0.007747121 |\n|    clip_fraction                | 0.0375      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -0.991      |\n|    explained_variance           | 0.135       |\n|    learning_rate                | 0.001       |\n|    loss                         | 185         |\n|    n_updates                    | 315         |\n|    policy_gradient_loss         | 0.00189     |\n|    value_loss                   | 317         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 79.5        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 26          |\n|    unit_deliver_ice_reward      | 0.0198      |\n|    unit_move_to_ice_reward      | 0.079       |\n|    water_produced               | 3.25        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 33.5        |\n| time/                           |             |\n|    fps                          | 654         |\n|    iterations                   | 107         |\n|    time_elapsed                 | 653         |\n|    total_timesteps              | 428000      |\n| train/                          |             |\n|    approx_kl                    | 0.040045965 |\n|    clip_fraction                | 0.165       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.24       |\n|    explained_variance           | 0.369       |\n|    learning_rate                | 0.001       |\n|    loss                         | 11.1        |\n|    n_updates                    | 318         |\n|    policy_gradient_loss         | 0.00447     |\n|    value_loss                   | 22.6        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 72          |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 26.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.085       |\n|    water_produced               | 4.75        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 33.5         |\n| time/                           |              |\n|    fps                          | 656          |\n|    iterations                   | 108          |\n|    time_elapsed                 | 658          |\n|    total_timesteps              | 432000       |\n| train/                          |              |\n|    approx_kl                    | 0.0020427795 |\n|    clip_fraction                | 0.00567      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.4         |\n|    explained_variance           | 0.14         |\n|    learning_rate                | 0.001        |\n|    loss                         | 70.4         |\n|    n_updates                    | 321          |\n|    policy_gradient_loss         | -3.3e-05     |\n|    value_loss                   | 150          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 71.5         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 32.5         |\n|    unit_deliver_ice_reward      | 0.013        |\n|    unit_move_to_ice_reward      | 0.083        |\n|    water_produced               | 5            |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 35.8        |\n| time/                           |             |\n|    fps                          | 657         |\n|    iterations                   | 109         |\n|    time_elapsed                 | 662         |\n|    total_timesteps              | 436000      |\n| train/                          |             |\n|    approx_kl                    | 0.011040736 |\n|    clip_fraction                | 0.1         |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.33       |\n|    explained_variance           | 0.187       |\n|    learning_rate                | 0.001       |\n|    loss                         | 51.8        |\n|    n_updates                    | 324         |\n|    policy_gradient_loss         | 0.0014      |\n|    value_loss                   | 127         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 69.8        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 14          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.086       |\n|    water_produced               | 2.88        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 39           |\n| time/                           |              |\n|    fps                          | 658          |\n|    iterations                   | 110          |\n|    time_elapsed                 | 667          |\n|    total_timesteps              | 440000       |\n| train/                          |              |\n|    approx_kl                    | 0.0034699335 |\n|    clip_fraction                | 0.106        |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.44        |\n|    explained_variance           | 0.187        |\n|    learning_rate                | 0.001        |\n|    loss                         | 17.7         |\n|    n_updates                    | 327          |\n|    policy_gradient_loss         | 0.00366      |\n|    value_loss                   | 34.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 73.3         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 47           |\n|    unit_deliver_ice_reward      | 0.00625      |\n|    unit_move_to_ice_reward      | 0.083        |\n|    water_produced               | 5.87         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 45.9        |\n| time/                           |             |\n|    fps                          | 660         |\n|    iterations                   | 111         |\n|    time_elapsed                 | 672         |\n|    total_timesteps              | 444000      |\n| train/                          |             |\n|    approx_kl                    | 0.008133476 |\n|    clip_fraction                | 0.0772      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.13       |\n|    explained_variance           | 0.11        |\n|    learning_rate                | 0.001       |\n|    loss                         | 143         |\n|    n_updates                    | 330         |\n|    policy_gradient_loss         | 0.000805    |\n|    value_loss                   | 259         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 74          |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 44.5        |\n|    unit_deliver_ice_reward      | 0.00675     |\n|    unit_move_to_ice_reward      | 0.0842      |\n|    water_produced               | 10          |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 44.2        |\n| time/                           |             |\n|    fps                          | 661         |\n|    iterations                   | 112         |\n|    time_elapsed                 | 677         |\n|    total_timesteps              | 448000      |\n| train/                          |             |\n|    approx_kl                    | 0.013449153 |\n|    clip_fraction                | 0.0634      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.12       |\n|    explained_variance           | 0.107       |\n|    learning_rate                | 0.001       |\n|    loss                         | 152         |\n|    n_updates                    | 333         |\n|    policy_gradient_loss         | 0.00197     |\n|    value_loss                   | 364         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 81.1        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 14.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.085       |\n|    water_produced               | 2.87        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 45.9        |\n| time/                           |             |\n|    fps                          | 662         |\n|    iterations                   | 113         |\n|    time_elapsed                 | 682         |\n|    total_timesteps              | 452000      |\n| train/                          |             |\n|    approx_kl                    | 0.009757179 |\n|    clip_fraction                | 0.0576      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.07       |\n|    explained_variance           | 0.0703      |\n|    learning_rate                | 0.001       |\n|    loss                         | 28.4        |\n|    n_updates                    | 336         |\n|    policy_gradient_loss         | 0.000825    |\n|    value_loss                   | 59.6        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 76          |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 43          |\n|    unit_deliver_ice_reward      | 0.0065      |\n|    unit_move_to_ice_reward      | 0.0855      |\n|    water_produced               | 9.38        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 35.6         |\n| time/                           |              |\n|    fps                          | 663          |\n|    iterations                   | 114          |\n|    time_elapsed                 | 687          |\n|    total_timesteps              | 456000       |\n| train/                          |              |\n|    approx_kl                    | 0.0009431959 |\n|    clip_fraction                | 0.00442      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.04        |\n|    explained_variance           | 0.196        |\n|    learning_rate                | 0.001        |\n|    loss                         | 101          |\n|    n_updates                    | 339          |\n|    policy_gradient_loss         | 0.000297     |\n|    value_loss                   | 197          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 75.3         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 33.5         |\n|    unit_deliver_ice_reward      | 0.02         |\n|    unit_move_to_ice_reward      | 0.0812       |\n|    water_produced               | 3.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 38.9         |\n| time/                           |              |\n|    fps                          | 664          |\n|    iterations                   | 115          |\n|    time_elapsed                 | 692          |\n|    total_timesteps              | 460000       |\n| train/                          |              |\n|    approx_kl                    | 0.0020565824 |\n|    clip_fraction                | 0.0124       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.15        |\n|    explained_variance           | 0.0313       |\n|    learning_rate                | 0.001        |\n|    loss                         | 107          |\n|    n_updates                    | 342          |\n|    policy_gradient_loss         | -3.03e-05    |\n|    value_loss                   | 216          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 72.5         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 37.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0895       |\n|    water_produced               | 9            |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 47.5        |\n| time/                           |             |\n|    fps                          | 665         |\n|    iterations                   | 116         |\n|    time_elapsed                 | 697         |\n|    total_timesteps              | 464000      |\n| train/                          |             |\n|    approx_kl                    | 0.005487441 |\n|    clip_fraction                | 0.0348      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.1        |\n|    explained_variance           | 0.0585      |\n|    learning_rate                | 0.001       |\n|    loss                         | 128         |\n|    n_updates                    | 345         |\n|    policy_gradient_loss         | -0.00058    |\n|    value_loss                   | 318         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 72.3        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 29          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0873      |\n|    water_produced               | 7.25        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 43           |\n| time/                           |              |\n|    fps                          | 666          |\n|    iterations                   | 117          |\n|    time_elapsed                 | 702          |\n|    total_timesteps              | 468000       |\n| train/                          |              |\n|    approx_kl                    | 0.0015139972 |\n|    clip_fraction                | 0.00742      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.14        |\n|    explained_variance           | 0.169        |\n|    learning_rate                | 0.001        |\n|    loss                         | 56.2         |\n|    n_updates                    | 348          |\n|    policy_gradient_loss         | 0.000198     |\n|    value_loss                   | 127          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 72.8         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 32.5         |\n|    unit_deliver_ice_reward      | 0.00625      |\n|    unit_move_to_ice_reward      | 0.0883       |\n|    water_produced               | 4.12         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 33.4         |\n| time/                           |              |\n|    fps                          | 667          |\n|    iterations                   | 118          |\n|    time_elapsed                 | 707          |\n|    total_timesteps              | 472000       |\n| train/                          |              |\n|    approx_kl                    | 0.0068646185 |\n|    clip_fraction                | 0.072        |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.27        |\n|    explained_variance           | 0.131        |\n|    learning_rate                | 0.001        |\n|    loss                         | 80.8         |\n|    n_updates                    | 351          |\n|    policy_gradient_loss         | 0.00158      |\n|    value_loss                   | 150          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 70.3         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 32           |\n|    unit_deliver_ice_reward      | 0.0128       |\n|    unit_move_to_ice_reward      | 0.0838       |\n|    water_produced               | 2.38         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 40.3        |\n| time/                           |             |\n|    fps                          | 668         |\n|    iterations                   | 119         |\n|    time_elapsed                 | 712         |\n|    total_timesteps              | 476000      |\n| train/                          |             |\n|    approx_kl                    | 0.003809554 |\n|    clip_fraction                | 0.0159      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.24       |\n|    explained_variance           | 0.0955      |\n|    learning_rate                | 0.001       |\n|    loss                         | 19.2        |\n|    n_updates                    | 354         |\n|    policy_gradient_loss         | -0.00034    |\n|    value_loss                   | 42.6        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 69          |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 62.5        |\n|    unit_deliver_ice_reward      | 0.0197      |\n|    unit_move_to_ice_reward      | 0.0817      |\n|    water_produced               | 12.8        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 47          |\n| time/                           |             |\n|    fps                          | 668         |\n|    iterations                   | 120         |\n|    time_elapsed                 | 717         |\n|    total_timesteps              | 480000      |\n| train/                          |             |\n|    approx_kl                    | 0.013279239 |\n|    clip_fraction                | 0.0882      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.1        |\n|    explained_variance           | 0.148       |\n|    learning_rate                | 0.001       |\n|    loss                         | 142         |\n|    n_updates                    | 357         |\n|    policy_gradient_loss         | 6.12e-05    |\n|    value_loss                   | 320         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 75.5        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 66          |\n|    unit_deliver_ice_reward      | 0.0265      |\n|    unit_move_to_ice_reward      | 0.0798      |\n|    water_produced               | 11.4        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 46.2         |\n| time/                           |              |\n|    fps                          | 669          |\n|    iterations                   | 121          |\n|    time_elapsed                 | 722          |\n|    total_timesteps              | 484000       |\n| train/                          |              |\n|    approx_kl                    | 0.0019535422 |\n|    clip_fraction                | 0.00742      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.05        |\n|    explained_variance           | 0.191        |\n|    learning_rate                | 0.001        |\n|    loss                         | 71.4         |\n|    n_updates                    | 360          |\n|    policy_gradient_loss         | 0.000319     |\n|    value_loss                   | 200          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 74.1         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 35.5         |\n|    unit_deliver_ice_reward      | 0.00675      |\n|    unit_move_to_ice_reward      | 0.0863       |\n|    water_produced               | 4.75         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 48.4         |\n| time/                           |              |\n|    fps                          | 670          |\n|    iterations                   | 122          |\n|    time_elapsed                 | 727          |\n|    total_timesteps              | 488000       |\n| train/                          |              |\n|    approx_kl                    | 0.0060512973 |\n|    clip_fraction                | 0.0812       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.21        |\n|    explained_variance           | 0.242        |\n|    learning_rate                | 0.001        |\n|    loss                         | 65.1         |\n|    n_updates                    | 363          |\n|    policy_gradient_loss         | 9.84e-05     |\n|    value_loss                   | 119          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 75.2         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 67.5         |\n|    unit_deliver_ice_reward      | 0.0138       |\n|    unit_move_to_ice_reward      | 0.0823       |\n|    water_produced               | 12.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 52.6         |\n| time/                           |              |\n|    fps                          | 671          |\n|    iterations                   | 123          |\n|    time_elapsed                 | 732          |\n|    total_timesteps              | 492000       |\n| train/                          |              |\n|    approx_kl                    | 0.0016000711 |\n|    clip_fraction                | 0.00592      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.16        |\n|    explained_variance           | 0.19         |\n|    learning_rate                | 0.001        |\n|    loss                         | 121          |\n|    n_updates                    | 366          |\n|    policy_gradient_loss         | -0.000543    |\n|    value_loss                   | 303          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 70.6         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 65.5         |\n|    unit_deliver_ice_reward      | 0.0273       |\n|    unit_move_to_ice_reward      | 0.0793       |\n|    water_produced               | 14.5         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 62.6        |\n| time/                           |             |\n|    fps                          | 672         |\n|    iterations                   | 124         |\n|    time_elapsed                 | 737         |\n|    total_timesteps              | 496000      |\n| train/                          |             |\n|    approx_kl                    | 0.003189516 |\n|    clip_fraction                | 0.0116      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.12       |\n|    explained_variance           | 0.148       |\n|    learning_rate                | 0.001       |\n|    loss                         | 193         |\n|    n_updates                    | 369         |\n|    policy_gradient_loss         | -0.000798   |\n|    value_loss                   | 473         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 74.5        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 44          |\n|    unit_deliver_ice_reward      | 0.0138      |\n|    unit_move_to_ice_reward      | 0.0845      |\n|    water_produced               | 6.63        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 47           |\n| time/                           |              |\n|    fps                          | 673          |\n|    iterations                   | 125          |\n|    time_elapsed                 | 742          |\n|    total_timesteps              | 500000       |\n| train/                          |              |\n|    approx_kl                    | 0.0007522622 |\n|    clip_fraction                | 0.00317      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.16        |\n|    explained_variance           | 0.166        |\n|    learning_rate                | 0.001        |\n|    loss                         | 101          |\n|    n_updates                    | 372          |\n|    policy_gradient_loss         | -0.000422    |\n|    value_loss                   | 215          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 72.2         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 33           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0885       |\n|    water_produced               | 7.38         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 49.9        |\n| time/                           |             |\n|    fps                          | 674         |\n|    iterations                   | 126         |\n|    time_elapsed                 | 747         |\n|    total_timesteps              | 504000      |\n| train/                          |             |\n|    approx_kl                    | 0.002014834 |\n|    clip_fraction                | 0.0306      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.21       |\n|    explained_variance           | 0.281       |\n|    learning_rate                | 0.001       |\n|    loss                         | 73          |\n|    n_updates                    | 375         |\n|    policy_gradient_loss         | -0.000465   |\n|    value_loss                   | 130         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 67.4        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 41          |\n|    unit_deliver_ice_reward      | 0.00675     |\n|    unit_move_to_ice_reward      | 0.0878      |\n|    water_produced               | 8.38        |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 100           |\n|    ep_rew_mean                  | 42.3          |\n| time/                           |               |\n|    fps                          | 675           |\n|    iterations                   | 127           |\n|    time_elapsed                 | 752           |\n|    total_timesteps              | 508000        |\n| train/                          |               |\n|    approx_kl                    | 0.00085238647 |\n|    clip_fraction                | 0.00125       |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.2          |\n|    explained_variance           | 0.178         |\n|    learning_rate                | 0.001         |\n|    loss                         | 53.2          |\n|    n_updates                    | 378           |\n|    policy_gradient_loss         | -0.000315     |\n|    value_loss                   | 103           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 67.2          |\n|    action_queue_updates_total   | 99            |\n|    ice_dug                      | 18.5          |\n|    unit_deliver_ice_reward      | 0             |\n|    unit_move_to_ice_reward      | 0.085         |\n|    water_produced               | 4.5           |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 48.6         |\n| time/                           |              |\n|    fps                          | 676          |\n|    iterations                   | 128          |\n|    time_elapsed                 | 757          |\n|    total_timesteps              | 512000       |\n| train/                          |              |\n|    approx_kl                    | 0.0072031417 |\n|    clip_fraction                | 0.0589       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.29        |\n|    explained_variance           | 0.222        |\n|    learning_rate                | 0.001        |\n|    loss                         | 83.8         |\n|    n_updates                    | 381          |\n|    policy_gradient_loss         | -0.000622    |\n|    value_loss                   | 162          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 66.4         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 40.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.088        |\n|    water_produced               | 10           |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 58.9        |\n| time/                           |             |\n|    fps                          | 677         |\n|    iterations                   | 129         |\n|    time_elapsed                 | 762         |\n|    total_timesteps              | 516000      |\n| train/                          |             |\n|    approx_kl                    | 0.008962757 |\n|    clip_fraction                | 0.0602      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.24       |\n|    explained_variance           | 0.153       |\n|    learning_rate                | 0.001       |\n|    loss                         | 134         |\n|    n_updates                    | 384         |\n|    policy_gradient_loss         | 0.00238     |\n|    value_loss                   | 244         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 66.9        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 55          |\n|    unit_deliver_ice_reward      | 0.0065      |\n|    unit_move_to_ice_reward      | 0.087       |\n|    water_produced               | 7.5         |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 68.8         |\n| time/                           |              |\n|    fps                          | 677          |\n|    iterations                   | 130          |\n|    time_elapsed                 | 767          |\n|    total_timesteps              | 520000       |\n| train/                          |              |\n|    approx_kl                    | 0.0051122727 |\n|    clip_fraction                | 0.0219       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.33        |\n|    explained_variance           | 0.174        |\n|    learning_rate                | 0.001        |\n|    loss                         | 157          |\n|    n_updates                    | 387          |\n|    policy_gradient_loss         | 0.000438     |\n|    value_loss                   | 329          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 71.4         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 36.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0873       |\n|    water_produced               | 8.88         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 47.3         |\n| time/                           |              |\n|    fps                          | 678          |\n|    iterations                   | 131          |\n|    time_elapsed                 | 771          |\n|    total_timesteps              | 524000       |\n| train/                          |              |\n|    approx_kl                    | 0.0016056054 |\n|    clip_fraction                | 0.0025       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.34        |\n|    explained_variance           | 0.0748       |\n|    learning_rate                | 0.001        |\n|    loss                         | 156          |\n|    n_updates                    | 390          |\n|    policy_gradient_loss         | 0.000244     |\n|    value_loss                   | 317          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 70           |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 17.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0875       |\n|    water_produced               | 3.88         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 44.4        |\n| time/                           |             |\n|    fps                          | 679         |\n|    iterations                   | 132         |\n|    time_elapsed                 | 777         |\n|    total_timesteps              | 528000      |\n| train/                          |             |\n|    approx_kl                    | 0.011055089 |\n|    clip_fraction                | 0.0899      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.4        |\n|    explained_variance           | 0.283       |\n|    learning_rate                | 0.001       |\n|    loss                         | 32.9        |\n|    n_updates                    | 393         |\n|    policy_gradient_loss         | 0.000244    |\n|    value_loss                   | 65.1        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 68          |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 27          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.089       |\n|    water_produced               | 5.75        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 50.2         |\n| time/                           |              |\n|    fps                          | 679          |\n|    iterations                   | 133          |\n|    time_elapsed                 | 782          |\n|    total_timesteps              | 532000       |\n| train/                          |              |\n|    approx_kl                    | 0.0044873105 |\n|    clip_fraction                | 0.0217       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.35        |\n|    explained_variance           | 0.329        |\n|    learning_rate                | 0.001        |\n|    loss                         | 46.5         |\n|    n_updates                    | 396          |\n|    policy_gradient_loss         | 0.000168     |\n|    value_loss                   | 101          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 67.3         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 39           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0878       |\n|    water_produced               | 7.88         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 99           |\n|    ep_rew_mean                  | 62.1         |\n| time/                           |              |\n|    fps                          | 680          |\n|    iterations                   | 134          |\n|    time_elapsed                 | 787          |\n|    total_timesteps              | 536000       |\n| train/                          |              |\n|    approx_kl                    | 0.0058946246 |\n|    clip_fraction                | 0.0247       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.39        |\n|    explained_variance           | 0.24         |\n|    learning_rate                | 0.001        |\n|    loss                         | 200          |\n|    n_updates                    | 399          |\n|    policy_gradient_loss         | 4.73e-06     |\n|    value_loss                   | 330          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 64.2         |\n|    action_queue_updates_total   | 96.6         |\n|    ice_dug                      | 46.3         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0898       |\n|    water_produced               | 11.3         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 99          |\n|    ep_rew_mean                  | 60.1        |\n| time/                           |             |\n|    fps                          | 681         |\n|    iterations                   | 135         |\n|    time_elapsed                 | 792         |\n|    total_timesteps              | 540000      |\n| train/                          |             |\n|    approx_kl                    | 0.022822138 |\n|    clip_fraction                | 0.175       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.35       |\n|    explained_variance           | 0.177       |\n|    learning_rate                | 0.001       |\n|    loss                         | 145         |\n|    n_updates                    | 402         |\n|    policy_gradient_loss         | 0.00684     |\n|    value_loss                   | 302         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 77          |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 73          |\n|    unit_deliver_ice_reward      | 0.027       |\n|    unit_move_to_ice_reward      | 0.0795      |\n|    water_produced               | 12.3        |\n-------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 99         |\n|    ep_rew_mean                  | 48.7       |\n| time/                           |            |\n|    fps                          | 682        |\n|    iterations                   | 136        |\n|    time_elapsed                 | 797        |\n|    total_timesteps              | 544000     |\n| train/                          |            |\n|    approx_kl                    | 0.01151202 |\n|    clip_fraction                | 0.104      |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -1.36      |\n|    explained_variance           | 0.307      |\n|    learning_rate                | 0.001      |\n|    loss                         | 56.4       |\n|    n_updates                    | 405        |\n|    policy_gradient_loss         | 0.000892   |\n|    value_loss                   | 125        |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 69.5       |\n|    action_queue_updates_total   | 99         |\n|    ice_dug                      | 33.5       |\n|    unit_deliver_ice_reward      | 0.007      |\n|    unit_move_to_ice_reward      | 0.0838     |\n|    water_produced               | 5.62       |\n------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 24.6        |\n| time/                           |             |\n|    fps                          | 683         |\n|    iterations                   | 137         |\n|    time_elapsed                 | 801         |\n|    total_timesteps              | 548000      |\n| train/                          |             |\n|    approx_kl                    | 0.011198047 |\n|    clip_fraction                | 0.0446      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.31       |\n|    explained_variance           | 0.18        |\n|    learning_rate                | 0.001       |\n|    loss                         | 55.1        |\n|    n_updates                    | 408         |\n|    policy_gradient_loss         | 0.00117     |\n|    value_loss                   | 123         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 67.4        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 13.5        |\n|    unit_deliver_ice_reward      | 0.0065      |\n|    unit_move_to_ice_reward      | 0.0828      |\n|    water_produced               | 1           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 28.4        |\n| time/                           |             |\n|    fps                          | 684         |\n|    iterations                   | 138         |\n|    time_elapsed                 | 806         |\n|    total_timesteps              | 552000      |\n| train/                          |             |\n|    approx_kl                    | 0.010079566 |\n|    clip_fraction                | 0.0759      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.48       |\n|    explained_variance           | 0.314       |\n|    learning_rate                | 0.001       |\n|    loss                         | 15.4        |\n|    n_updates                    | 411         |\n|    policy_gradient_loss         | 0.000556    |\n|    value_loss                   | 31.8        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 69.2        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 30.5        |\n|    unit_deliver_ice_reward      | 0.00675     |\n|    unit_move_to_ice_reward      | 0.081       |\n|    water_produced               | 5.87        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 35           |\n| time/                           |              |\n|    fps                          | 684          |\n|    iterations                   | 139          |\n|    time_elapsed                 | 812          |\n|    total_timesteps              | 556000       |\n| train/                          |              |\n|    approx_kl                    | 0.0011899645 |\n|    clip_fraction                | 0.0005       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.44        |\n|    explained_variance           | 0.344        |\n|    learning_rate                | 0.001        |\n|    loss                         | 53.9         |\n|    n_updates                    | 414          |\n|    policy_gradient_loss         | -0.000148    |\n|    value_loss                   | 116          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 67.2         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 33           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.087        |\n|    water_produced               | 7.12         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 40.8         |\n| time/                           |              |\n|    fps                          | 685          |\n|    iterations                   | 140          |\n|    time_elapsed                 | 816          |\n|    total_timesteps              | 560000       |\n| train/                          |              |\n|    approx_kl                    | 0.0013199823 |\n|    clip_fraction                | 0.00225      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.42        |\n|    explained_variance           | 0.214        |\n|    learning_rate                | 0.001        |\n|    loss                         | 38.3         |\n|    n_updates                    | 417          |\n|    policy_gradient_loss         | 9.71e-05     |\n|    value_loss                   | 81.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 67.4         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 23.5         |\n|    unit_deliver_ice_reward      | 0.00675      |\n|    unit_move_to_ice_reward      | 0.085        |\n|    water_produced               | 4.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 48.8         |\n| time/                           |              |\n|    fps                          | 686          |\n|    iterations                   | 141          |\n|    time_elapsed                 | 821          |\n|    total_timesteps              | 564000       |\n| train/                          |              |\n|    approx_kl                    | 0.0061535556 |\n|    clip_fraction                | 0.0221       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.48        |\n|    explained_variance           | 0.164        |\n|    learning_rate                | 0.001        |\n|    loss                         | 83.1         |\n|    n_updates                    | 420          |\n|    policy_gradient_loss         | -0.000404    |\n|    value_loss                   | 167          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 65.6         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 40           |\n|    unit_deliver_ice_reward      | 0.00675      |\n|    unit_move_to_ice_reward      | 0.0823       |\n|    water_produced               | 9.25         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 58.3         |\n| time/                           |              |\n|    fps                          | 687          |\n|    iterations                   | 142          |\n|    time_elapsed                 | 826          |\n|    total_timesteps              | 568000       |\n| train/                          |              |\n|    approx_kl                    | 0.0054101446 |\n|    clip_fraction                | 0.0218       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.52        |\n|    explained_variance           | 0.296        |\n|    learning_rate                | 0.001        |\n|    loss                         | 100          |\n|    n_updates                    | 423          |\n|    policy_gradient_loss         | 0.000659     |\n|    value_loss                   | 184          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 70           |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 47.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0823       |\n|    water_produced               | 9.75         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 48.6        |\n| time/                           |             |\n|    fps                          | 688         |\n|    iterations                   | 143         |\n|    time_elapsed                 | 831         |\n|    total_timesteps              | 572000      |\n| train/                          |             |\n|    approx_kl                    | 0.015977148 |\n|    clip_fraction                | 0.0703      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.62       |\n|    explained_variance           | 0.332       |\n|    learning_rate                | 0.001       |\n|    loss                         | 96.3        |\n|    n_updates                    | 426         |\n|    policy_gradient_loss         | 0.00113     |\n|    value_loss                   | 185         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 69.7        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 28.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0795      |\n|    water_produced               | 6.12        |\n-------------------------------------------------\nEval num_timesteps=576000, episode_reward=339.89 +/- 625.47\nEpisode length: 311.00 +/- 20.00\n-------------------------------------------------\n| eval/                           |             |\n|    mean_ep_length               | 311         |\n|    mean_reward                  | 340         |\n| time/                           |             |\n|    total_timesteps              | 576000      |\n| train/                          |             |\n|    approx_kl                    | 0.002726303 |\n|    clip_fraction                | 0.00633     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.82       |\n|    explained_variance           | 0.411       |\n|    learning_rate                | 0.001       |\n|    loss                         | 57.9        |\n|    n_updates                    | 429         |\n|    policy_gradient_loss         | -0.000904   |\n|    value_loss                   | 123         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 63.6        |\n|    action_queue_updates_total   | 94.5        |\n|    ice_dug                      | 12.4        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.079       |\n|    water_produced               | 3.1         |\n-------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 98.1     |\n|    ep_rew_mean     | 42.9     |\n| time/              |          |\n|    fps             | 640      |\n|    iterations      | 144      |\n|    time_elapsed    | 899      |\n|    total_timesteps | 576000   |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 98.1         |\n|    ep_rew_mean                  | 35.6         |\n| time/                           |              |\n|    fps                          | 641          |\n|    iterations                   | 145          |\n|    time_elapsed                 | 904          |\n|    total_timesteps              | 580000       |\n| train/                          |              |\n|    approx_kl                    | 0.0037902698 |\n|    clip_fraction                | 0.0529       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.83        |\n|    explained_variance           | 0.334        |\n|    learning_rate                | 0.001        |\n|    loss                         | 36.1         |\n|    n_updates                    | 432          |\n|    policy_gradient_loss         | 0.00147      |\n|    value_loss                   | 66           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 69.6         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 22           |\n|    unit_deliver_ice_reward      | 0.00675      |\n|    unit_move_to_ice_reward      | 0.079        |\n|    water_produced               | 4.5          |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 99            |\n|    ep_rew_mean                  | 37.5          |\n| time/                           |               |\n|    fps                          | 641           |\n|    iterations                   | 146           |\n|    time_elapsed                 | 910           |\n|    total_timesteps              | 584000        |\n| train/                          |               |\n|    approx_kl                    | 0.00076431606 |\n|    clip_fraction                | 0.000667      |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.8          |\n|    explained_variance           | 0.327         |\n|    learning_rate                | 0.001         |\n|    loss                         | 113           |\n|    n_updates                    | 435           |\n|    policy_gradient_loss         | 0.000323      |\n|    value_loss                   | 198           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 70.3          |\n|    action_queue_updates_total   | 99            |\n|    ice_dug                      | 32.5          |\n|    unit_deliver_ice_reward      | 0.00675       |\n|    unit_move_to_ice_reward      | 0.0787        |\n|    water_produced               | 7.63          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 44.2         |\n| time/                           |              |\n|    fps                          | 642          |\n|    iterations                   | 147          |\n|    time_elapsed                 | 915          |\n|    total_timesteps              | 588000       |\n| train/                          |              |\n|    approx_kl                    | 0.0028938893 |\n|    clip_fraction                | 0.00842      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.78        |\n|    explained_variance           | 0.352        |\n|    learning_rate                | 0.001        |\n|    loss                         | 55.4         |\n|    n_updates                    | 438          |\n|    policy_gradient_loss         | 7.32e-05     |\n|    value_loss                   | 109          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 69.4         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 19.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0773       |\n|    water_produced               | 4.62         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 31.5        |\n| time/                           |             |\n|    fps                          | 643         |\n|    iterations                   | 148         |\n|    time_elapsed                 | 920         |\n|    total_timesteps              | 592000      |\n| train/                          |             |\n|    approx_kl                    | 0.010847624 |\n|    clip_fraction                | 0.0667      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.65       |\n|    explained_variance           | 0.127       |\n|    learning_rate                | 0.001       |\n|    loss                         | 74.3        |\n|    n_updates                    | 441         |\n|    policy_gradient_loss         | 0.00114     |\n|    value_loss                   | 134         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 66.1        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 10.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0738      |\n|    water_produced               | 2.63        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 36.5        |\n| time/                           |             |\n|    fps                          | 644         |\n|    iterations                   | 149         |\n|    time_elapsed                 | 924         |\n|    total_timesteps              | 596000      |\n| train/                          |             |\n|    approx_kl                    | 0.009385851 |\n|    clip_fraction                | 0.102       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.87       |\n|    explained_variance           | 0.254       |\n|    learning_rate                | 0.001       |\n|    loss                         | 19.4        |\n|    n_updates                    | 444         |\n|    policy_gradient_loss         | -0.000973   |\n|    value_loss                   | 48.1        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 63.5        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 20.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.079       |\n|    water_produced               | 4.75        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 45.2         |\n| time/                           |              |\n|    fps                          | 645          |\n|    iterations                   | 150          |\n|    time_elapsed                 | 929          |\n|    total_timesteps              | 600000       |\n| train/                          |              |\n|    approx_kl                    | 0.0056860046 |\n|    clip_fraction                | 0.0204       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.222        |\n|    learning_rate                | 0.001        |\n|    loss                         | 65.4         |\n|    n_updates                    | 447          |\n|    policy_gradient_loss         | -0.00224     |\n|    value_loss                   | 128          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 65.8         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 59           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0845       |\n|    water_produced               | 11.3         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 62.5        |\n| time/                           |             |\n|    fps                          | 646         |\n|    iterations                   | 151         |\n|    time_elapsed                 | 934         |\n|    total_timesteps              | 604000      |\n| train/                          |             |\n|    approx_kl                    | 0.017703557 |\n|    clip_fraction                | 0.0914      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.58       |\n|    explained_variance           | 0.341       |\n|    learning_rate                | 0.001       |\n|    loss                         | 153         |\n|    n_updates                    | 450         |\n|    policy_gradient_loss         | 0.00392     |\n|    value_loss                   | 291         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 73.3        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 52          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0858      |\n|    water_produced               | 12.7        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 53.9         |\n| time/                           |              |\n|    fps                          | 646          |\n|    iterations                   | 152          |\n|    time_elapsed                 | 940          |\n|    total_timesteps              | 608000       |\n| train/                          |              |\n|    approx_kl                    | 0.0037260435 |\n|    clip_fraction                | 0.0176       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.6         |\n|    explained_variance           | 0.25         |\n|    learning_rate                | 0.001        |\n|    loss                         | 170          |\n|    n_updates                    | 453          |\n|    policy_gradient_loss         | 5.62e-05     |\n|    value_loss                   | 393          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 70.4         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 21.5         |\n|    unit_deliver_ice_reward      | 0.00675      |\n|    unit_move_to_ice_reward      | 0.0808       |\n|    water_produced               | 4.25         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 53.4         |\n| time/                           |              |\n|    fps                          | 647          |\n|    iterations                   | 153          |\n|    time_elapsed                 | 944          |\n|    total_timesteps              | 612000       |\n| train/                          |              |\n|    approx_kl                    | 0.0023060907 |\n|    clip_fraction                | 0.00867      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.68        |\n|    explained_variance           | 0.442        |\n|    learning_rate                | 0.001        |\n|    loss                         | 92.4         |\n|    n_updates                    | 456          |\n|    policy_gradient_loss         | -0.000231    |\n|    value_loss                   | 157          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 69           |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 38           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0853       |\n|    water_produced               | 9.25         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 61.6        |\n| time/                           |             |\n|    fps                          | 648         |\n|    iterations                   | 154         |\n|    time_elapsed                 | 949         |\n|    total_timesteps              | 616000      |\n| train/                          |             |\n|    approx_kl                    | 0.004893051 |\n|    clip_fraction                | 0.0156      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.49       |\n|    explained_variance           | 0.22        |\n|    learning_rate                | 0.001       |\n|    loss                         | 112         |\n|    n_updates                    | 459         |\n|    policy_gradient_loss         | -0.00229    |\n|    value_loss                   | 198         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 69.2        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 49          |\n|    unit_deliver_ice_reward      | 0.00675     |\n|    unit_move_to_ice_reward      | 0.081       |\n|    water_produced               | 11.4        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 61.6        |\n| time/                           |             |\n|    fps                          | 649         |\n|    iterations                   | 155         |\n|    time_elapsed                 | 954         |\n|    total_timesteps              | 620000      |\n| train/                          |             |\n|    approx_kl                    | 0.009584994 |\n|    clip_fraction                | 0.113       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.46       |\n|    explained_variance           | 0.252       |\n|    learning_rate                | 0.001       |\n|    loss                         | 237         |\n|    n_updates                    | 462         |\n|    policy_gradient_loss         | -0.000231   |\n|    value_loss                   | 558         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 69.1        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 39.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0848      |\n|    water_produced               | 8           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 44.1        |\n| time/                           |             |\n|    fps                          | 650         |\n|    iterations                   | 156         |\n|    time_elapsed                 | 958         |\n|    total_timesteps              | 624000      |\n| train/                          |             |\n|    approx_kl                    | 0.003644326 |\n|    clip_fraction                | 0.0466      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.58       |\n|    explained_variance           | 0.0773      |\n|    learning_rate                | 0.001       |\n|    loss                         | 87          |\n|    n_updates                    | 465         |\n|    policy_gradient_loss         | 2.13e-06    |\n|    value_loss                   | 174         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 68.5        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 13.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0825      |\n|    water_produced               | 2.87        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 33.7        |\n| time/                           |             |\n|    fps                          | 651         |\n|    iterations                   | 157         |\n|    time_elapsed                 | 963         |\n|    total_timesteps              | 628000      |\n| train/                          |             |\n|    approx_kl                    | 0.004700082 |\n|    clip_fraction                | 0.0344      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.62       |\n|    explained_variance           | 0.526       |\n|    learning_rate                | 0.001       |\n|    loss                         | 21.7        |\n|    n_updates                    | 468         |\n|    policy_gradient_loss         | 0.00116     |\n|    value_loss                   | 47          |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 66.4        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 23.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0862      |\n|    water_produced               | 4.62        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 38           |\n| time/                           |              |\n|    fps                          | 652          |\n|    iterations                   | 158          |\n|    time_elapsed                 | 968          |\n|    total_timesteps              | 632000       |\n| train/                          |              |\n|    approx_kl                    | 0.0074348086 |\n|    clip_fraction                | 0.0277       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.41        |\n|    explained_variance           | 0.199        |\n|    learning_rate                | 0.001        |\n|    loss                         | 45.4         |\n|    n_updates                    | 471          |\n|    policy_gradient_loss         | -0.00122     |\n|    value_loss                   | 96.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 66.9         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 33           |\n|    unit_deliver_ice_reward      | 0.00675      |\n|    unit_move_to_ice_reward      | 0.0825       |\n|    water_produced               | 7.62         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 41.9         |\n| time/                           |              |\n|    fps                          | 652          |\n|    iterations                   | 159          |\n|    time_elapsed                 | 974          |\n|    total_timesteps              | 636000       |\n| train/                          |              |\n|    approx_kl                    | 0.0060564564 |\n|    clip_fraction                | 0.0217       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.26        |\n|    explained_variance           | 0.314        |\n|    learning_rate                | 0.001        |\n|    loss                         | 62.9         |\n|    n_updates                    | 474          |\n|    policy_gradient_loss         | -0.00161     |\n|    value_loss                   | 130          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 71.5         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 24.5         |\n|    unit_deliver_ice_reward      | 0.007        |\n|    unit_move_to_ice_reward      | 0.087        |\n|    water_produced               | 5.38         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 38.1         |\n| time/                           |              |\n|    fps                          | 653          |\n|    iterations                   | 160          |\n|    time_elapsed                 | 978          |\n|    total_timesteps              | 640000       |\n| train/                          |              |\n|    approx_kl                    | 0.0021119346 |\n|    clip_fraction                | 0.0108       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.16        |\n|    explained_variance           | 0.23         |\n|    learning_rate                | 0.001        |\n|    loss                         | 28.5         |\n|    n_updates                    | 477          |\n|    policy_gradient_loss         | -0.000381    |\n|    value_loss                   | 61.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 73.1         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 41           |\n|    unit_deliver_ice_reward      | 0.033        |\n|    unit_move_to_ice_reward      | 0.0737       |\n|    water_produced               | 5.88         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 53.8         |\n| time/                           |              |\n|    fps                          | 654          |\n|    iterations                   | 161          |\n|    time_elapsed                 | 983          |\n|    total_timesteps              | 644000       |\n| train/                          |              |\n|    approx_kl                    | 0.0022893557 |\n|    clip_fraction                | 0.00983      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.16        |\n|    explained_variance           | 0.187        |\n|    learning_rate                | 0.001        |\n|    loss                         | 61.6         |\n|    n_updates                    | 480          |\n|    policy_gradient_loss         | -0.000464    |\n|    value_loss                   | 164          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 69.8         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 65.5         |\n|    unit_deliver_ice_reward      | 0.00575      |\n|    unit_move_to_ice_reward      | 0.0858       |\n|    water_produced               | 12.1         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 67.9        |\n| time/                           |             |\n|    fps                          | 655         |\n|    iterations                   | 162         |\n|    time_elapsed                 | 988         |\n|    total_timesteps              | 648000      |\n| train/                          |             |\n|    approx_kl                    | 0.016612317 |\n|    clip_fraction                | 0.1         |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.11       |\n|    explained_variance           | 0.156       |\n|    learning_rate                | 0.001       |\n|    loss                         | 168         |\n|    n_updates                    | 483         |\n|    policy_gradient_loss         | 0.00277     |\n|    value_loss                   | 407         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 69.7        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 36          |\n|    unit_deliver_ice_reward      | 0.00675     |\n|    unit_move_to_ice_reward      | 0.0883      |\n|    water_produced               | 8.75        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 41.9        |\n| time/                           |             |\n|    fps                          | 656         |\n|    iterations                   | 163         |\n|    time_elapsed                 | 993         |\n|    total_timesteps              | 652000      |\n| train/                          |             |\n|    approx_kl                    | 0.010192616 |\n|    clip_fraction                | 0.0731      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.11       |\n|    explained_variance           | 0.188       |\n|    learning_rate                | 0.001       |\n|    loss                         | 69.9        |\n|    n_updates                    | 486         |\n|    policy_gradient_loss         | -0.000755   |\n|    value_loss                   | 139         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 67.9        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 14          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0857      |\n|    water_produced               | 1.88        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 38.2        |\n| time/                           |             |\n|    fps                          | 656         |\n|    iterations                   | 164         |\n|    time_elapsed                 | 998         |\n|    total_timesteps              | 656000      |\n| train/                          |             |\n|    approx_kl                    | 0.013389352 |\n|    clip_fraction                | 0.0776      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.28       |\n|    explained_variance           | 0.276       |\n|    learning_rate                | 0.001       |\n|    loss                         | 23          |\n|    n_updates                    | 489         |\n|    policy_gradient_loss         | 0.00286     |\n|    value_loss                   | 46.8        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 64.9        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 23          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.088       |\n|    water_produced               | 5.12        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 41.6        |\n| time/                           |             |\n|    fps                          | 657         |\n|    iterations                   | 165         |\n|    time_elapsed                 | 1003        |\n|    total_timesteps              | 660000      |\n| train/                          |             |\n|    approx_kl                    | 0.006665381 |\n|    clip_fraction                | 0.0218      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.21       |\n|    explained_variance           | 0.158       |\n|    learning_rate                | 0.001       |\n|    loss                         | 39.1        |\n|    n_updates                    | 492         |\n|    policy_gradient_loss         | -0.00126    |\n|    value_loss                   | 79.1        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 65.7        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 26.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0895      |\n|    water_produced               | 6           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 57.5         |\n| time/                           |              |\n|    fps                          | 658          |\n|    iterations                   | 166          |\n|    time_elapsed                 | 1008         |\n|    total_timesteps              | 664000       |\n| train/                          |              |\n|    approx_kl                    | 0.0028222413 |\n|    clip_fraction                | 0.0122       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.11        |\n|    explained_variance           | 0.285        |\n|    learning_rate                | 0.001        |\n|    loss                         | 42.9         |\n|    n_updates                    | 495          |\n|    policy_gradient_loss         | 0.000416     |\n|    value_loss                   | 93.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 65.4         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 48           |\n|    unit_deliver_ice_reward      | 0.0065       |\n|    unit_move_to_ice_reward      | 0.0872       |\n|    water_produced               | 8.62         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 57.7         |\n| time/                           |              |\n|    fps                          | 659          |\n|    iterations                   | 167          |\n|    time_elapsed                 | 1013         |\n|    total_timesteps              | 668000       |\n| train/                          |              |\n|    approx_kl                    | 0.0019946522 |\n|    clip_fraction                | 0.00817      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.12        |\n|    explained_variance           | 0.281        |\n|    learning_rate                | 0.001        |\n|    loss                         | 87.6         |\n|    n_updates                    | 498          |\n|    policy_gradient_loss         | 0.00161      |\n|    value_loss                   | 174          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 68           |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 31.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.087        |\n|    water_produced               | 7            |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 45.7        |\n| time/                           |             |\n|    fps                          | 659         |\n|    iterations                   | 168         |\n|    time_elapsed                 | 1018        |\n|    total_timesteps              | 672000      |\n| train/                          |             |\n|    approx_kl                    | 0.004034795 |\n|    clip_fraction                | 0.0552      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.24       |\n|    explained_variance           | 0.23        |\n|    learning_rate                | 0.001       |\n|    loss                         | 76.8        |\n|    n_updates                    | 501         |\n|    policy_gradient_loss         | 0.000486    |\n|    value_loss                   | 264         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 67.1        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 29.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0865      |\n|    water_produced               | 6.12        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 40.9        |\n| time/                           |             |\n|    fps                          | 660         |\n|    iterations                   | 169         |\n|    time_elapsed                 | 1023        |\n|    total_timesteps              | 676000      |\n| train/                          |             |\n|    approx_kl                    | 0.001810736 |\n|    clip_fraction                | 0.0136      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.22       |\n|    explained_variance           | 0.299       |\n|    learning_rate                | 0.001       |\n|    loss                         | 56.3        |\n|    n_updates                    | 504         |\n|    policy_gradient_loss         | -8.21e-05   |\n|    value_loss                   | 110         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 65.4        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 18.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0853      |\n|    water_produced               | 4           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 43.9         |\n| time/                           |              |\n|    fps                          | 661          |\n|    iterations                   | 170          |\n|    time_elapsed                 | 1027         |\n|    total_timesteps              | 680000       |\n| train/                          |              |\n|    approx_kl                    | 0.0069349245 |\n|    clip_fraction                | 0.0332       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.13        |\n|    explained_variance           | 0.137        |\n|    learning_rate                | 0.001        |\n|    loss                         | 42.7         |\n|    n_updates                    | 507          |\n|    policy_gradient_loss         | 0.000417     |\n|    value_loss                   | 86.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 63.5         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 39.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0872       |\n|    water_produced               | 6.25         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 44          |\n| time/                           |             |\n|    fps                          | 662         |\n|    iterations                   | 171         |\n|    time_elapsed                 | 1032        |\n|    total_timesteps              | 684000      |\n| train/                          |             |\n|    approx_kl                    | 0.011144476 |\n|    clip_fraction                | 0.101       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.16       |\n|    explained_variance           | 0.313       |\n|    learning_rate                | 0.001       |\n|    loss                         | 53.6        |\n|    n_updates                    | 510         |\n|    policy_gradient_loss         | 0.000156    |\n|    value_loss                   | 127         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 67          |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 23          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0867      |\n|    water_produced               | 4.87        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 47.8         |\n| time/                           |              |\n|    fps                          | 662          |\n|    iterations                   | 172          |\n|    time_elapsed                 | 1038         |\n|    total_timesteps              | 688000       |\n| train/                          |              |\n|    approx_kl                    | 0.0013509437 |\n|    clip_fraction                | 0.0123       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.19        |\n|    explained_variance           | 0.238        |\n|    learning_rate                | 0.001        |\n|    loss                         | 42.2         |\n|    n_updates                    | 513          |\n|    policy_gradient_loss         | 0.00047      |\n|    value_loss                   | 81.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 63.8         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 38           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0893       |\n|    water_produced               | 8.75         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 42.2         |\n| time/                           |              |\n|    fps                          | 663          |\n|    iterations                   | 173          |\n|    time_elapsed                 | 1043         |\n|    total_timesteps              | 692000       |\n| train/                          |              |\n|    approx_kl                    | 0.0019611984 |\n|    clip_fraction                | 0.00983      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.08        |\n|    explained_variance           | 0.284        |\n|    learning_rate                | 0.001        |\n|    loss                         | 62.4         |\n|    n_updates                    | 516          |\n|    policy_gradient_loss         | -0.00044     |\n|    value_loss                   | 122          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 65.1         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 18           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0857       |\n|    water_produced               | 3.75         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 38.4         |\n| time/                           |              |\n|    fps                          | 664          |\n|    iterations                   | 174          |\n|    time_elapsed                 | 1047         |\n|    total_timesteps              | 696000       |\n| train/                          |              |\n|    approx_kl                    | 0.0019632531 |\n|    clip_fraction                | 0.0345       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.19        |\n|    explained_variance           | 0.373        |\n|    learning_rate                | 0.001        |\n|    loss                         | 33.7         |\n|    n_updates                    | 519          |\n|    policy_gradient_loss         | 0.000658     |\n|    value_loss                   | 65.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 68           |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 18.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0855       |\n|    water_produced               | 3            |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 34.8        |\n| time/                           |             |\n|    fps                          | 664         |\n|    iterations                   | 175         |\n|    time_elapsed                 | 1052        |\n|    total_timesteps              | 700000      |\n| train/                          |             |\n|    approx_kl                    | 0.002047828 |\n|    clip_fraction                | 0.00983     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.15       |\n|    explained_variance           | 0.045       |\n|    learning_rate                | 0.001       |\n|    loss                         | 22.4        |\n|    n_updates                    | 522         |\n|    policy_gradient_loss         | 5.27e-05    |\n|    value_loss                   | 49.9        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 64.6        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 35.5        |\n|    unit_deliver_ice_reward      | 0.013       |\n|    unit_move_to_ice_reward      | 0.0828      |\n|    water_produced               | 4.62        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 46.8        |\n| time/                           |             |\n|    fps                          | 665         |\n|    iterations                   | 176         |\n|    time_elapsed                 | 1057        |\n|    total_timesteps              | 704000      |\n| train/                          |             |\n|    approx_kl                    | 0.003668657 |\n|    clip_fraction                | 0.0132      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.06       |\n|    explained_variance           | 0.176       |\n|    learning_rate                | 0.001       |\n|    loss                         | 85.2        |\n|    n_updates                    | 525         |\n|    policy_gradient_loss         | 0.000277    |\n|    value_loss                   | 151         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 68          |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 33.5        |\n|    unit_deliver_ice_reward      | 0.0138      |\n|    unit_move_to_ice_reward      | 0.0847      |\n|    water_produced               | 6.75        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 46.7         |\n| time/                           |              |\n|    fps                          | 666          |\n|    iterations                   | 177          |\n|    time_elapsed                 | 1062         |\n|    total_timesteps              | 708000       |\n| train/                          |              |\n|    approx_kl                    | 0.0068329135 |\n|    clip_fraction                | 0.0362       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.12        |\n|    explained_variance           | 0.237        |\n|    learning_rate                | 0.001        |\n|    loss                         | 64.9         |\n|    n_updates                    | 528          |\n|    policy_gradient_loss         | 0.00224      |\n|    value_loss                   | 127          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 67.1         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 24           |\n|    unit_deliver_ice_reward      | 0.0065       |\n|    unit_move_to_ice_reward      | 0.0827       |\n|    water_produced               | 5.37         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 42           |\n| time/                           |              |\n|    fps                          | 666          |\n|    iterations                   | 178          |\n|    time_elapsed                 | 1068         |\n|    total_timesteps              | 712000       |\n| train/                          |              |\n|    approx_kl                    | 0.0089575825 |\n|    clip_fraction                | 0.0467       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.4         |\n|    explained_variance           | 0.36         |\n|    learning_rate                | 0.001        |\n|    loss                         | 50.5         |\n|    n_updates                    | 531          |\n|    policy_gradient_loss         | -0.00196     |\n|    value_loss                   | 120          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 64.6         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 8.5          |\n|    unit_deliver_ice_reward      | 0.00675      |\n|    unit_move_to_ice_reward      | 0.08         |\n|    water_produced               | 1.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 36.8         |\n| time/                           |              |\n|    fps                          | 667          |\n|    iterations                   | 179          |\n|    time_elapsed                 | 1073         |\n|    total_timesteps              | 716000       |\n| train/                          |              |\n|    approx_kl                    | 0.0023319747 |\n|    clip_fraction                | 0.00692      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.82        |\n|    explained_variance           | 0.659        |\n|    learning_rate                | 0.001        |\n|    loss                         | 17.7         |\n|    n_updates                    | 534          |\n|    policy_gradient_loss         | -0.000299    |\n|    value_loss                   | 36.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 66.3         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 25           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0755       |\n|    water_produced               | 5.88         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 32.2        |\n| time/                           |             |\n|    fps                          | 667         |\n|    iterations                   | 180         |\n|    time_elapsed                 | 1077        |\n|    total_timesteps              | 720000      |\n| train/                          |             |\n|    approx_kl                    | 0.014251632 |\n|    clip_fraction                | 0.14        |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.73       |\n|    explained_variance           | 0.321       |\n|    learning_rate                | 0.001       |\n|    loss                         | 151         |\n|    n_updates                    | 537         |\n|    policy_gradient_loss         | 0.00511     |\n|    value_loss                   | 264         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 64.8        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 13.5        |\n|    unit_deliver_ice_reward      | 0.0065      |\n|    unit_move_to_ice_reward      | 0.08        |\n|    water_produced               | 2           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 30.3         |\n| time/                           |              |\n|    fps                          | 668          |\n|    iterations                   | 181          |\n|    time_elapsed                 | 1082         |\n|    total_timesteps              | 724000       |\n| train/                          |              |\n|    approx_kl                    | 0.0034587034 |\n|    clip_fraction                | 0.0305       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.73        |\n|    explained_variance           | 0.398        |\n|    learning_rate                | 0.001        |\n|    loss                         | 17.5         |\n|    n_updates                    | 540          |\n|    policy_gradient_loss         | 0.00054      |\n|    value_loss                   | 45.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 65.5         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 12.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0825       |\n|    water_produced               | 2.37         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 26.6        |\n| time/                           |             |\n|    fps                          | 669         |\n|    iterations                   | 182         |\n|    time_elapsed                 | 1087        |\n|    total_timesteps              | 728000      |\n| train/                          |             |\n|    approx_kl                    | 0.002190711 |\n|    clip_fraction                | 0.0123      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.59       |\n|    explained_variance           | 0.462       |\n|    learning_rate                | 0.001       |\n|    loss                         | 20.8        |\n|    n_updates                    | 543         |\n|    policy_gradient_loss         | 0.000579    |\n|    value_loss                   | 49.5        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 64.4        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 20          |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0828      |\n|    water_produced               | 4           |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 100         |\n|    ep_rew_mean                  | 32.9        |\n| time/                           |             |\n|    fps                          | 669         |\n|    iterations                   | 183         |\n|    time_elapsed                 | 1092        |\n|    total_timesteps              | 732000      |\n| train/                          |             |\n|    approx_kl                    | 0.001581133 |\n|    clip_fraction                | 0.00225     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.46       |\n|    explained_variance           | 0.398       |\n|    learning_rate                | 0.001       |\n|    loss                         | 27.2        |\n|    n_updates                    | 546         |\n|    policy_gradient_loss         | 0.000156    |\n|    value_loss                   | 57          |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 64.6        |\n|    action_queue_updates_total   | 99          |\n|    ice_dug                      | 21.5        |\n|    unit_deliver_ice_reward      | 0           |\n|    unit_move_to_ice_reward      | 0.0845      |\n|    water_produced               | 4.75        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 38.8         |\n| time/                           |              |\n|    fps                          | 670          |\n|    iterations                   | 184          |\n|    time_elapsed                 | 1098         |\n|    total_timesteps              | 736000       |\n| train/                          |              |\n|    approx_kl                    | 0.0038245644 |\n|    clip_fraction                | 0.0124       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.38        |\n|    explained_variance           | 0.102        |\n|    learning_rate                | 0.001        |\n|    loss                         | 124          |\n|    n_updates                    | 549          |\n|    policy_gradient_loss         | 0.000623     |\n|    value_loss                   | 250          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 64.8         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 19           |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0817       |\n|    water_produced               | 4.12         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 100          |\n|    ep_rew_mean                  | 33.9         |\n| time/                           |              |\n|    fps                          | 670          |\n|    iterations                   | 185          |\n|    time_elapsed                 | 1103         |\n|    total_timesteps              | 740000       |\n| train/                          |              |\n|    approx_kl                    | 0.0025928523 |\n|    clip_fraction                | 0.0075       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.3         |\n|    explained_variance           | 0.179        |\n|    learning_rate                | 0.001        |\n|    loss                         | 46           |\n|    n_updates                    | 552          |\n|    policy_gradient_loss         | 0.000341     |\n|    value_loss                   | 89.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 65.2         |\n|    action_queue_updates_total   | 99           |\n|    ice_dug                      | 13.5         |\n|    unit_deliver_ice_reward      | 0            |\n|    unit_move_to_ice_reward      | 0.0807       |\n|    water_produced               | 3.38         |\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Process ForkServerProcess-12:\nProcess ForkServerProcess-11:\nProcess ForkServerProcess-9:\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nKeyboardInterrupt\nProcess ForkServerProcess-2:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nProcess ForkServerProcess-3:\nProcess ForkServerProcess-4:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nProcess ForkServerProcess-10:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nProcess ForkServerProcess-1:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nProcess ForkServerProcess-6:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nProcess ForkServerProcess-7:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 28, in _worker\n    cmd, data = remote.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nProcess ForkServerProcess-8:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 30, in _worker\n    observation, reward, done, info = env.step(data)\n  File \"/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/monitor.py\", line 94, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/opt/conda/lib/python3.7/site-packages/gym/wrappers/time_limit.py\", line 18, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/tmp/ipykernel_4730/3780088508.py\", line 50, in step\n  File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 289, in step\n    return self.env.step(action)\n  File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 323, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/opt/conda/lib/python3.7/site-packages/luxai_s2/wrappers/sb3.py\", line 119, in step\n    obs, reward, done, info = self.env.step(lux_action)\n  File \"/opt/conda/lib/python3.7/site-packages/gym/wrappers/time_limit.py\", line 18, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/opt/conda/lib/python3.7/site-packages/luxai_s2/env.py\", line 880, in step\n    self.state.board.lichen -= 1\nKeyboardInterrupt\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_4730/2795605237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mparse_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_4730/2795605237.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_4730/3780088508.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m    248\u001b[0m     model.learn(\n\u001b[1;32m    249\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensorboardCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train_metrics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     )\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"My code is still raw but I'm ready to hear your advice for improving it or you can improve it on your own.","metadata":{}}]}